\Section{extraction}{Intra-Clause Open IE}

% Intro
For this section, we assume that our sentences consist of a single clause --
  that is, a single open IE relation to extract.
\refsec{clause} will describe how to split a longer sentence into such clauses.
The task then becomes generating a maximally compact sentence which retains
  the core semantics of the original utterance, and parsing the sentence
  into a conventional open IE subject verb object triple.

\Subsection{natlog}{Natural Logic}
% Intro
For the first of these, we adopt a subset of natural logic semantics dictating
  contexts in which lexical items can be removed.
Natural logic, as a formalism, attempts to capture common logical inferences
  appealing directly to the syntax of language, rather than parsing to a
  specialized logical syntax.
It provides a proof theory for lexical mutations to a sentence which either
  preserve or negate the truth of the premise.

% Teaser on natural logic
For instance, if \ww{all rabbits eat vegetables} then 
  \ww{all cute rabbits eat vegetables}, since we are allowed to mutate
  the lexical item \ww{rabbit} to \ww{cute rabbit}.
This is done by observing that \ww{rabbit} is in scope of the first
  argument to the operator \ww{all}.
Since \ww{all} is \textit{antitone} in its first argument, we are allowed
  to replace \ww{rabbit} with an item which is more specific -- in this case
  \ww{cute rabbit}.
To contrast, the operator \ww{some} is \textit{monotone} in its first argument,
  and therefore we may derive the inference from the more specific
  \ww{cute rabbit} to the more general \ww{rabbit} in:
  \ww{some cute rabbits are small} therefore \ww{some rabbits are small}.

% Wrap up teaser
For a more comprehensive introduction to natural logic, the reader may be
  refered to \newcite{key:2008vanbenthem-natlog}.
In this paper, we make use of only the theory on when deletions in a sentence
  are warranted.
For this, we mark the scopes of all operators (\ww{all}, \ww{no}, \ww{many},
  etc.) in a sentence, and from this determine whether every lexical item
  can be replaced by something more general (has upwards \textit{polarity}),
  more specific (downwards polarity), or neither.
In the absense of operators, all items have upwards polarity.

% Deleting edges as up/down the hierarchy
Each dependency arc is then classified into whether deleting the subtree
  eminating from that arc makes the lexical item at that node more general,
  more specific (a rare case), or neither.
We make use of the Stanford Dependencies representation 
  \cite{key:stanford-dependencies}.
For example, removing the \typ{amod} edge in \ww{cute rabbit} yields the
  more general lexical item \ww{rabbit}.
However, removing the \typ{nsubj} edge in \ww{Fido runs} would yield the
  incomparable phrase \ww{runs}.
The last, rare, case is an edge that causes the resulting item to be 
  more specific -- e.g., \typ{quantmod}: 
  \ww{about 200} is more general than \ww{200}.

% This is approximate
Although for many dependency edges this is a good approximation,
  there are at least two cases where more attention is warranted.
The first of these concerns privative adjectives: for example a \ww{fake gun}
  is not a gun.
For this case, we make use of the list of privative adjectives collected
  in \newcite{key:2014nayak-adjectives}.
The second concern is with prepositional attachment, and direct object
  edges.

% PP Attachment
For example, whereas \ww{Alice went to the playground with Bob} entails
  that \ww{Alice went to the playground}, it is not meaningful to infer that
  \ww{Alice is friends with Bob} entails \ww{Alice is friends}.
Analogously, \ww{Alice played baseball on Sunday} entails that \ww{Alice played
  on Sunday}; but, \ww{Obama signed a bill on Sunday} should not entail the
  awkward phrase \ww{?Obama signed on Sunday}.

% Learning attachment affinity
We learn these attachment affinities empirically from the syntactic n-grams
  corpus of \newcite{key:2013goldberg-syngrams}.
This gives us counts for how often object and preposition edges occur in the
  context of (1) the governing verb, and (2) the other prepositional (or
  object) outgoing edges.
We hypothesize that edges which are frequently seen to co-occur 
  (e.g., \ww{is friends} and \ww{friends with}) are likely
  to be essential to the meaning of the sentence.
To this end, we compute the following probabilities:

% Backoff order
\begin{enumerate}
  \item The probability of seeing a PP edge, given the governing verb
        and verb's direct object.
  \item The probability of seeing a PP edge, given the governing verb
        and a neighboring PP edge.
  \item The probability of seeing a PP edge, given the governing verb.
  \item The probability of seeing a particular object, given the governing
        verb and a PP edge.
  \item The probability of seeing any object, given the governing
        verb and a PP edge.
\end{enumerate}

% Inference time
At inference time, we then back off to the most informative probability
  given the context we are considering, in the order listed above.
For example, if we are deciding whether to remove the prepositional phrase
  in \ww{Alice went to the playground with Bob}, we would compute
  the probability of seeing \ww{with} given the governing verb
  \ww{go} and the neighboring preposition \ww{to}.
Calling this probability $p$, the score $s$ of an extraction removing the edge
  in question becomes:
\begin{equation*}
  s = 1 - \min(1, \frac{p}{K})
\end{equation*}
where $K$ is a hyperparameter denoting the minimum fraction of the time an
  edge should occur in a context to be considered entirely unremovable.
In our experiments, we set $K=\frac{1}{3}$.


\Subsection{patterns}{Atomic Patterns}
% Verb versus Nominal relations
The second half of the triple extraction process is the segmenting of a
  sentence into a subject verb object triple.
We identify two types of relations commonly extracted by open IE systems:
  those that center around a verb, and those that center around a noun.
For example, \ww{cats have tails} versus \ww{Google CEO Larry Page}.

\begin{table}
\begin{tabular}{l|l}
\textbf{Input} & \textbf{Extraction} \\
\hline
\ww{\small{cats play with yarn}}        & \small{(cats; play with; yarn)} \\
\ww{\small{fish like to swim}}          & \small{(fish; like to; swim)} \\
\ww{\small{cats have tails}}            & \small{(cats; have; tails)} \\
\ww{\small{Durin, son of Thorin}}       & \small{(Durin; is son of; Thorin)} \\
\ww{\small{Thorin's son, Durin}}        & \small{(Durin; is son of; Thorin)} \\
\ww{\small{cats are cute}}              & \small{(cats; are; cute)} \\
\ww{\small{Tom and Jerry are fighting}} & \small{(Tom; fighting; Jerry)} \\
\ww{\small{There are cats with tails}}  & \small{(cats; have; tails)}
\end{tabular}
\caption{\label{tab:patterns}
  The eight atomic patterns used to segment a short sentence into an open IE
  triple.
}
\end{table}

% The verb patterns
The first of these we capture with 8 simple hand-coded patterns.
Sample extractions for each of the patterns are given in \reftab{patterns}.
The second of these we capture with a mix of 3 dependency and 3 surface
  form patterns \cite{key:stanford-tokensregex}.
All six of these capture either variants of the declarative \ww{be} relation
  (e.g., adjective modification) or simple variants
  (e.g., \ww{Napoleon of France} entails \ww{Napoleon is of France}).

% Restrict NER types
However, for all of these patterns we restrict both the subject and the object
  to named entities.
This (1) allows us to find sub-structure in noun phrases
  (\ww{president Barack Obama} entails \ww{Barack Obama is president},
  but not \ww{Obama is Barack}); and
  (2) restricts the number of context specific extractions
  (\ww{ferocious cats} does not entail that \ww{cats are ferocious} in a
  general context).

