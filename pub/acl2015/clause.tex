\Section{clause}{Inter-Clause Open IE}

\begin{figure*}
\begin{center}
\begin{tabular}{cc}
  % Full tree
  \begin{dependency}[text only label, label style={above}]
    \begin{deptext}[column sep=-0.1cm]
      Born \& in \& Honolulu \& , \& Hawaii \& , \& Obama \& is \& a \&
        graduate \& of \& Columbia \\
    \end{deptext}
    \depedge[edge unit distance=1.0ex]{1}{5}{prep\_in}
    \depedge[edge unit distance=1.0ex]{5}{3}{nn}
    \depedge[edge unit distance=1.0ex, edge style={darkred!60!black,thick,densely dotted}]{10}{1}{\textbf{\darkred{vmod}}}
    \depedge[edge unit distance=2.0ex, edge style={blue!60!black,thick}]{10}{7}{\darkblue{nsubj}}
    \depedge[edge unit distance=1.75ex]{10}{8}{cop}
    \depedge[edge unit distance=1.0ex]{10}{9}{det}
    \depedge[edge unit distance=1.0ex]{10}{12}{prep\_of}
  \end{dependency}
  &
  % Just Clause
  \begin{dependency}[text only label, label style={above}]
    \begin{deptext}[column sep=-0.1cm]
      Obama \& Born \& in \& Honolulu \& , \& Hawaii \\
    \end{deptext}
    \depedge[edge unit distance=1.0ex]{2}{6}{prep\_in}
    \depedge[edge unit distance=1.0ex]{6}{4}{nn}
    \depedge[edge unit distance=2.0ex, edge style={blue!60!black,thick}]{2}{1}{\darkblue{nsubj}}
  \end{dependency}
  \\
  (a) & (b)
\end{tabular}
\end{center}
\caption{\label{fig:clausesplit}
A sample sentence illustrating splitting a clause while copying the parent's
  subject.
(a) The original tree.
(b) The tree with the clause \ww{Born in Honolulu, Hawaii} split out, with
  the borrowed subject.
}
\end{figure*}



% Introduce task
In the first stage of our method, we produce a set of self-contained clauses
  from a longer utterance.
The objective is to produce a set of clauses which can stand on their own
  syntactically and semantically, and are entailed by original sentence.
For example, taking the example sentence in \reffig{clausesplit} the sentence:
\begin{quotation}
\noindent\ww{Born in Honolulu, Hawaii, Obama is a graduate of Columbia.}
\end{quotation}
should produce two clauses:
\begin{quotation}
\noindent\ww{Obama born in Honolulu, Hawaii.} \\
\noindent\ww{Obama is a graduate of Columbia.}
\end{quotation}

% Formal description
We treat this task as structured prediction problem.
At each node of the parse tree $p$, for each outgoing dependency arc going
  from this parent node $p$ to a child node $c$ with label $l$:
  $e = p \xrightarrow{l} c$, we define a set of actions we can perform.
This action is decomposed into two parts: (1) the action to take on the outgoing
  edge ($e$), and (2) the action to take on the
  parent node ($p$).
For example, in our motivating example, we are considering the arc:
  $e = \ww{graduate} \xrightarrow{vmod} \ww{born}$.
In this case, the correct action is to
  (1) split the sentence on the edge $e$ to create a new clause,
  and (2) steal the subject of \ww{graduate}.

% Motivation
We can view the actions in our clause splitter as ensuring that our
  clauses are both \textit{short} and \textit{coherent}.
Splitting off clauses on dependency edges shortens a sentence, but does not
  always form a coherent constituent (e.g., \ww{born in Honolulu, Hawaii}).
However, for a wide range of cases, this incoherence can be fixed by copying
  part of the parent node.
We proceed to describe this action space in more detail, followed by an
  explanation of our training data, and finally our classifier.

%
% Action Space
%
\Subsection{actions}{Action Space}
% The actions
The three actions we can perform on a dependency edge are:

\paragraph{Stop} 
  Do not recurse on this arc, as the subtree under this arc is
    not entailed by the parent sentence.
  This is the case, for example, for most leaf nodes
    (\ww{furry cats are cute} should not entail the clause \ww{furry});
    but, also the case when the clause would not be entailed by the original
    sentence (e.g., \ww{Dentists do not suggest that you should eat rocks}
    should not entail that \ww{you should eat rocks}).

\paragraph{Recurse}
  Recurse on this dependency arc, but do not yield it as a new clause.
  This is often useful for long sentences, where a relevant clause is nested
    within a constituent that should not itself be a clause.
  For example, in the sentence \ww{Obama, our 44$^\textrm{th}$ president, is
    the first African-American to hold the office}:
    we would like to extract the clause 
    \ww{Obama is our 44$^\textrm{th}$ president}, but not the intermediate
    clause \ww{Obama, our 44$^\textrm{th}$ president}.

\paragraph{Split}
  Split a new clause on this dependency arc.
  A canonical case of this action could be \textit{ccomp} arc from \ww{suggest}
    to \ww{brush} in \ww{Dentists suggest that you should brush your teeth}.

For the \textbf{Stop} action, we do not need to further specify an action
  to take on the parent node.
However, for both of the other actions, it is often the case that we would like
  to borrow some portion of the parent tree.
We define two such common actions -- though this is not necessarily an
  exhaustive list of possible actions:

\paragraph{Clone Subject} 
  If the arc we are considering is not a subject arc,
    we copy the subject of the parent node and attach it as a subject of the
    child node.
  This is the action taken in the example
    \ww{Born in Honolulu, Hawaii, Obama graduated from Columbia}.

\paragraph{Clone Root} 
  If the arc we are taking is the only outgoing arc from a node, we clone the
    parent node as the (passive) subject of the child.
  This is the action taken in the example
    \ww{Obama, our 44$^\textrm{th}$ president} to yield a clause with the
    semantics of \ww{Obama [is] our 44$^\textrm{th}$ president}.

% Additional actions
Although additional actions are easy to imagine, we found empirically that
  these cover a wide range of applicable cases.
We now turn our attention to the training data for learning these actions.

%
% Training Data
%
\Subsection{distsup}{Training Data}

% Distant supervision
We make use of distant supervision for training our clause splitting model.
Distant supervision is a method for training a supervised classifier using only
  a large unlabeled corpus, and a knowledge base of known facts.
We can then generate noisy sentence-based annotations, by assuming that every
  sentence which contains two entities in our knowledge base 
This, then, can give us a noisy corpus of sentences with known relations 
  annotated in them.

% Why KBP data?
We use a small subset of the KBP source corpora for 
  2010 \cite{key:2010ji-kbpoverview}
  and 2013 \cite{key:2013surdeanu-kbpoverview}
  as our distantly supervised corpus.
To try to maximize the density of known relations in the training sentences,
  we take all sentences from the source corpora which have at least one known
  relation for every 10 tokens in the sentence.
This results in \num{43155} sentences.
In addition, we incorporate the 23 thousand manually annotated examples
  from \newcite{key:2014angeli-active}.
Although we train our clause splitter on KBP data, we do not incorporate
  anything particular about the KBP relation set.
Our only assumption is that clauses which contain a KBP relation occur in
  a similar distribution as clauses which contain arbitrary open-domain
  relations.

% Distant supervision x 2
Once we are given a collection of labeled sentences, we make make a second
  assumption, akin to distant supervision for clause splitting.
We assume that a sequence of actions which leads to a correct extraction of
  a known relation is a \textit{positive path}.
A sequence of actions which results in a clause which produces no relations
  is a \textit{negative path}.
This is, however, only distantly supervised training data.
It's possible for an incorrect sequence of actions to produce the correct
  extraction, or even more commonly for a correct sequence of actions to 
  produce no extraction.

% Relation to Distant supervision #1
Furthermore, we still have to mitigate the noise in the distantly supervised
  method for generating our annotated sentences.
In particular, the problem of \textit{incomplete negatives}: there are sequences
  of actions which produce a relation, but it is not the known relation in the
  sentence.\needcite
This however does not inherently mean that it is an incorrect relation -- perhaps
  it is a relation we don't know about in the knowledge base.
In the interest of erring on the side of higher recall -- particularly given
  a dataset already heavily biased towards the \textit{not a clause} action --
  we discard such paths.

% Handling of examples
Given a set of positive and negative \textit{paths}, we construct training
  data for our action classifier.
All but the last action in a positive path are added to the training set
  with the label \textbf{Recurse}; the last action is added with the label
  \textbf{Split}.
Only the last action in a negative path is added with the label \textbf{Stop}.
We partition the feature space of our dataset according to the action
  applied to the parent node.

%
% Classifier
%
\Subsection{clauseclassifier}{Classifier and Inference}

\begin{table}
\begin{tabular}{ll}
  \textbf{Feature Class} & \textbf{Feature Templates} \\
  \hline
  Edge taken          & $\{ l, \textrm{short\_name}(l) \}$ \\
  Last edge taken     & $\{ \textrm{incoming\_edge}(p) \}$ \\
  Neighbors of parent & $\{ \textrm{nbr}(e), (e, \textrm{nbr}(e)) \}$ \\
  Grandchild edges    & $\{ \textrm{out\_edge}(c), $ \\
                      & $~~ (e, \textrm{out\_edge}(c)) \}$ \\
  Grandchild count    & $\{ \textrm{count}\left( \textrm{nbr}(e_\textrm{child}) \right) $ \\
                      & $~~ \left(e, \textrm{count}\left( \textrm{nbr}(e_\textrm{child}) \right) \right) \}$ \\
  Has subject/object  & $\forall_{e \in \{e, e_\textrm{child}\}} \forall_{l \in \{\textit{subj}, \textit{obj}\}} $ \\
                      & $~~ \1(l \in \textrm{nbr}(e)) $ \\
  POS tag signature   & $\{ \textrm{pos}(p), \textrm{pos}(c), $ \\
                      & $~~ \left( \textrm{pos}(p), \textrm{pos}(c) \right) \}$ \\
  Features at root    & $\{ \1(p = root), \textrm{POS}(p) \}$
\end{tabular}
\caption{\label{tab:features}
Features for the clause splitter model, deciding to split on the arc
  $e = p \xrightarrow{l} c$.
The feature class is a high level description of features; the feature
  templates are the particular templates used.
For instance, the POS signature contains the tag of the parent, the tag of
  the child, and both tags joined in a single feature.
Note that all features are joined with the action to be taken on the parent
  node.
}
\end{table}

% Classifier
We train a multinomial logistic regression classifier on our training data,
  using the features in \reftab{features}.
The most salient features are the label of the edge being taken, the
  incoming edge to the parent of the edge being taken, neighboring edges
  for both the parent and child of the edge, and the part of speech tag of
  the endpoints of the edge.
The dataset is weighted to give $3x$ weight to examples in the \textbf{Recurse}
  class, as precision errors in this class are relatively harmless for accuracy,
  while recall errors are directly harmful to recall.

% Introduce performance
We report informal performance numbers for our classifier.
Note that the performance ceiling for the task is far below 100\%.
Precision errors are inevitable due to the incompleteness of our
  knowledge base (we find relations which are correct but not yet known);
  recall errors are in turn often due to the noise of distant supervision.
% Statistics
Nonetheless, we report precision and recall
  over 5-fold cross validation on a dataset of
  \num{1130849} datums -- of which \num{50217} have label \textbf{Split} and
  \num{54778} have label \textbf{Recurse}.
The \textbf{Stop} action is taken to be the negative class.
% Performance
For the \textbf{Split} class, we achieve a precision of 74.2, with a recall
  of 55.4 (63.4 F$_1$)
For the \textbf{Recurse} class, we achieve a precision of 38.0, with a recall
  of 60.6 (46.7 F$_1$)

% Inference
Inference now reduces to a search problem.
Beginning at the root of the tree, we consider every outgoing edge.
  For every possible action to be performed on the parent (i.e., clone subject,
  clone root, no action), we apply our trained classifier to determine
  whether we 
  (1) split the edge off as a clause, and recurse;
  (2) do not split the edge, and recurse; or 
  (3) do not recurse.
In the first two cases, we recurse on the child of the arc, and continue until
  either all arcs have been exhausted, or all remaining candidate arcs
  have been marked as not recursable.

