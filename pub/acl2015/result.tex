\Section{result}{Evaluation}
% Introduce KBP
We evaluate our approach in the context of an end-to-end relation extraction
  task -- the TAC KBP Slot Filling challenge.
In Slot Filling, we are given a large unlabeled corpus of text, a fixed
  schema of relations (see \refsec{mapping}), and a set of
  query entities.
The task is to find all relation triples in the corpus that have as a subject
  the query entity, and as a relation one of the defined relations.
This can be viewed as the task of automatically filling in structured information
  -- like Wikipedia Infoboxes -- from a large unstructured corpus of text.
% Motivate KBP
We use the KBP evaluation to motivate open IE in a real-world, end-to-end task
  where it can compete with a range of other approaches.

% Comparison
We compare our approach to the University of Washington submission to
  TAC-KBP 2013 \cite{key:2013soderland-kbp}.
This system, similar to ours, employed OpenIE v4.0 (a successor to Ollie) 
  run over
  the KBP corpus and then generated a mapping from the extracted relations to
  the fixed schema.

% Differences
Unlike our system Open IE v4.0 employs a semantic role
  component extracting structured SRL frames, alongside a conventional
  open IE system.
%Our system does not make use of such structured information.
Furthermore, the UW submission allowed for extracting relations and entities
  from substrings of an open IE triple argument.
For example, from the triple \textit{(Smith; was appointed; acting director of Acme
  Corporation)}, they extract that Smith is employed by Acme Corporation.
We disallow such extractions, passing the burden of finding correct
  precise extractions to the open IE system itself (see \refsec{extraction}).
Lastly, it should be noted that our system is penalized when it extracts
  correct facts that no other team in the competition found.

% Misc differences
Both the UW submission and our system make use of an entity linker
  and a coreference system.
For entity linking, the UW submission made use of Tom Lin's entity linker
  \cite{key:2012lin-el}; our submission uses the Illinois Wikifier 
  \cite{key:2011ratinov-el} without the relational inference component, for
  efficiency.
For coreference, UW makes use of the Stanford coreference system
  \cite{key:stanford-coref}; we employ a variant of the simple coref system
  described in \cite{key:2014pink-kbp}, which removes the need for syntactic
  parsing and was shown to not reduce end-to-end recall by a significant
  amount.

\begin{table}
\begin{center}
\begin{tabular}{l|cc:c}
\textbf{System} & \textbf{P} & \textbf{R} & \textbf{F$_1$} \\
\hline
UW Official         & \textbf{69.75} & 11.38 & 19.56 \\
Ollie               & ??.?? & ??.?? & ??.?? \\
\hline
Our System          & 59.05 & 12.75 & 20.97 \\
$~~$ + Alt. Names & 55.51 & \textbf{16.59} & \textbf{25.54} \\
\end{tabular}
\end{center}
\caption{\label{tab:results}
A summary of our results on the end-to-end KBP Slot Filling task.
We compare against an official submission to the challenge using
  Open IE v4.0 (UW Official), as well as the performance of our system
  using Ollie or Exemplar as the backing open IE system.
}
\end{table}

% Report results (KBP)
We report our results in \reftab{results}.
We report two versions of our system: one making use of alternate names
  via coref and entity linking, and another using only the relations from
  our system.
The second of these is a more fair comparision to the UW Official score,
  which does not make use of coreference for alternate names.
All results are reported with the \texttt{anydoc} flag set to true
  in the evaluation script, meaning that only the truth of the extracted
  knowledge base entry and not the associated provenance is scored.
%In absence of human evaluators, this is in order to not penalize our 
%  system unfairly for extracting a new correct provenance.

% Report results (ablation)
In addition, we run our system with our relation extraction component replaced
  with Ollie \cite{key:2012mausam-ollie} run over the same corpus.
These offer a more isolated comparison between the systems, removing
  other conflating factors in the evaluation.

% Demo
Lastly, we make a demo of our system publicly available online at
  \url{http://128.12.224.119/openie/}.
