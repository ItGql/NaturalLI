\documentclass[11pt,a4paper]{article}
% -- IMPORTS --
% (lib)
%\usepackage{acl2013}
\usepackage{times,latexsym,amsfonts,amssymb,amsmath,graphicx,url,bbm,rotating}
\usepackage{multirow,hhline,natbib,bussproofs}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[ruled,lined,linesnumbered]{algorithm2e}
% (custom)
\input std-macros.tex
\input ../macros.tex
% (tweaks)
\definecolor{darkred}{rgb}{0.5451, 0.0, 0.0}
\definecolor{darkgreen}{rgb}{0.0, 0.3922, 0.0}

\def\w#1{\textit{#1}}


\title{Cats have Tails}
\author{ 
}
\date{}

% -- CONTENT --
\begin{document}

\maketitle


\Section{search}{Search Problem}
We run a search over mutations over specific words of the input candidate
  consequent.
The search problem is given a consequent as input.
This is composed of the following:

\begin{itemize}
  \item A sentence, tokenized into coherent logical entities.
        For instance, \textit{dog} and \textit{cat} would be entities;
          but, so would \textit{George W. Bush} -- despite the latter covering
          multiple words.
  \item A monotonicity marking for each of these words.
        This is one of: upward monotone, downward monotone, or flat (no
          monotonicity information could be gathered).
  \item The relevant sense of the word.
        This is a WordNet synset when it's available; if not, this is
          set to a special sense denoting no synset.
        If a WordNet synset is available, it will never be set to the
          `no synset' category.
\end{itemize}

The paths from the result of this searches are used
  as features in the learning problem
  (see \refsec{learn}).

We define the problem by specifying the state space,
  the valid transitions, along with the weights of
  the transitions.
The algorithm used is a Uniform Cost Search, leading
  to the additional challenge of ensuring that the
  cost of each transition is very fast -- that is,
  on the order of microseconds to keep pace with
  the memory accesses for the search.

%Before delving into the search problem, we define the
%  notion of a \textbf{node}.
%A node is, at a high level, a phrase such as one
%  which would correspond to an argument or a relation.
%It is:
%\begin{itemize}
%\item Case sensitive, but normalized to be either
%      capitalized, all capitals, or all lowercase.
%\item Lemmatized and normalized where appropriate.
%      For instance, \w{has} and \w{had} are the same
%      node. In particular, the ReVerb normalized
%      relation is used.
%\item In the case when the phrase maps exactly to a
%      word form in WordNet, it is the WordNet synset.
%\end{itemize}
%
%The definition of the search problem is:

\Subsection{state}{State}
A state in the graph is defined as a partial path from the consequent to a
  valid antecedent.
That is, a state is primarily parameterized by its candidate fact which may be
  in the knowledge base.
In addition, some auxilliary information is also tracked in a search state:

\begin{itemize}
  \item The fact this fact mutated from.
        This is necessary to reconstruct the final path.
  \item The monotonicity marking of each word in the fact.
        This is necessary for the weights, to ensure that monotonicity is
          respected.
  \item The synset of each word in the fact.
        This is necessary to compute the valid mutations of that edge, according
          to the WordNet hierarchy.
  \item A bitmask denoting whether a word has been modified in the past.
        This is set when a word has been modified, and then another word is
          subsequently modified.
        This ensures that edits on a word are contiguous.
        We do not edit a particualr word, and then edit another word, and then
          perform more edits on the initial word.
\end{itemize}

\Subsection{weights}{Weights}
\todo{This section is now wrong}
We can denote a state transition as $(n_{-1}, n)$ -- 
  corresponding to a state $n$ and the previous state $n-1$.
The weight of a transition from $(n_{-1},n)$ to
  $(n,n_{+1})$ is given by appealing to the
  transitions between $n_{-1}$ and $n$, and between
  $n$ and $n_{+1}$.

We denote the type of transition 
  (e.g., ins($w$) or freebase($r$))
  between $n_{-1}$ and $n$ to be $\phi_{-1}$, and the
  type of transition between $n$ and $n_{+1}$ to
  be $\phi_0$.
For both of these, we define weights $w_{\phi_{-1}}$
  and $w_{\phi_0}$, as well as a weight for the
  bigram of the two: $w_{\phi_{-1}, \phi_0}$.
Lastly, we incur a cost for exiting the domain of natural
  logic entailments: $w_{nle}$.
Our cost is thus given by:

\begin{equation}
\textrm{cost} = - w_{\phi_{0}}
                - w_{\phi_{-1}, \phi_0}
                - \1(\textrm{broke entailment}) \cdot w_{nle}
\end{equation}

We constrain $w_{\phi} \leq 0$ to ensure that the
  cost is always positive. 

Note, furthermore, that this is a natural decomposition
  of a log-linear model where $\phi$ denote features
  and $w$ denote the weights of the features.
The cost of a path will become:

\begin{equation}
\sum_i cost_i = - \sum_i \left[ w_{\phi_i)} + w_{\phi_{i-1}, \phi_i}\right]
\end{equation}

If we exponentiate the negative of this, we arrive at:

\begin{equation}
\exp( \sum_i cost_i )
  = \exp\left( \sum_i \left[ w_{\phi_i)} + w_{\phi_{i-1}, \phi_i} \right] \right)
\end{equation}

Over two classes: \textit{true} and \textit{false};
  and, defining notation where $\phi$ denotes the
  vector of active path types taken and $w$ denotes
  the global weight vector, we arrive at our
  log-linear model:

\begin{equation}
P(\textit{true}) \propto e^{w^{T} \phi}
\end{equation}

This decomposition is important, as it allows our
  search to get smarter along with our learning
  algorithm, and allows us to find better support
  for facts as we learn what patterns entail
  ``good'' support.

%\Subsection{boundaries}{Start and Terminal States}
%The start state is the relevant word from the fact
%  we are beginning from, normalized to a valid node
%  (i.e., lemmatized, properly cased, etc.).
%For example, the search from $a_1$ of \w{cats have tails}
%  would be \w{cat}.
%
%The terminal state of the search is any node which
%  is attached to a known fact on the correct term.
%For example, a search ffrom $a_1$ of \w{cats have tails}
%  would finish at \w{dog} if there is a fact in the
%  database such as \w{dogs have tails}.


\Subsection{transitions}{Transitions}
A valid transition is one between states
  $(n_{-1}, n)$ and $(n, n_{+1})$, such that in
  relation to $n$, $n_{+1}$ is one of:

\begin{itemize}
\item \textbf{Add/Remove a quantifier}.
      This is a class of transitions denoted by
      \textit{ins($w$)} or \textit{del($w$)} 
      where $w$ is the word being added or removed.
      For example, transitioning from \w{cat} to
      \w{a cat} or \w{every cat} or visa versa.
\item \textbf{Add/Remove an adjective}.
      Similar to above, but with adjectives. These are distinguished
      from the above in that they have meaning for natural logic
      entailment.
      \todo{how do we make this efficient?}
\item \textbf{WordNet hypernymy}.
      This is a class of transitions denoted by
      \textit{hyper} or \textit{hypo},
      For example, we could transition from \w{cat}
      to \w{feline} and eventually to \w{animal}
\item \textbf{WordNet relations}.
      This is a class of transitions related to the other
      WordNet relations (e.g., antonymy), primarily to
      capture some of the inferences in natlog.
\item \textbf{Freebase relations}.
      This is a class of transitions aimed primarily
      for proper nouns, traversing Freebase relations.
      These are denoted by \textit{freebase($r$)},
      where $r$ denotes the freebase relation we
      have traversed.
      For example, \w{Barack Obama} could transition
      to \w{USA} via the \textit{employee\_of}
      relation.
\item \textbf{ReVerb relation}.
      As in the Freebase case, we can move along a
      ReVerb relation, denoted as \textit{reverb($r$)}.
\item \textbf{Acronym and de-acronym}.
      Expanding or creating an acronym,denoted by
      \textit{acronym} and \textit{deacronym}.
\item \textbf{Nearest neighbor similarity}.
      The fallback is to search for nearest neighbors in similarity space.
      This is, roughly, the equivalent of the CoNLL paper.
\item \textbf{Drop sense}. Drop the sense of a word -- this is necessary to
      begin operating in nearest neighbors space, or even with Freebase, etc.
      relations.
\item \textbf{Infer sense}. Go from a sense-less word to one of its word senses.
      For example, chosing a particular sense for \textit{cat} from the
        initially senseless definition.
\end{itemize}

\Section{learn}{Learning Problem}

The prediction problem is a binary prediction task: is the given fact true
  or false, given a database of known facts.
We divide this section into the model, the objective function, and the
  data (or lack thereof).

\Subsection{model}{Model}
Given a query fact $(a_1, r, a_2)$ we define the \textbf{support} for that
  fact as the set of facts $\{ (a_1', r', a_2') \}$ such that we have
  paths from $a_1 \rightarrow a_1'$, $r \rightarrow n'$,
  and $a_2 \rightarrow a_2'$.
We are thus given a set of triples of paths $\{p_{a_1}, p_{r}, p_{a_2}\}$.
When featurized, 

\Subsection{objective}{Objective}


\Section{misc}{Misc Snippets}
\Subsection{jc}{Generalization of JC Similarity}
Let us assume we have words $w_1$ and $w_2$, with a least common subsumer $\textrm{lcs}$.
The JC distance $\textrm{dist}_{\textrm{jc}}(w_1, w_2)$ is:

\begin{equation}
\textrm{dist}_{\textrm{jc}}(w_1, w_2)
  = \log\frac{p(\textrm{lcs})^2}{p(w_1)p(w_2)}
\end{equation}

We show that our search over the Wordnet hierarchy generalizes this similarity.
In particular, let us define two features, $\phi_\uparrow$ and $\phi_\downarrow$,
  corresponding to going up and down the WordNet hierarchy, respectively.
Traversing the Wordnet hierarchy from words $w \rightarrow w'$ thus fires the $\phi$
  features with counts:

\begin{align}
  \phi_\uparrow(w \rightarrow w')
    &= \log\frac{p(w')}{p(w)} = \log p(w') - \log p(w) \\
  \phi_\downarrow(w \rightarrow w')
    &= \log\frac{p(w)}{p(w')} = \log p(w) - \log p(w') 
\end{align}

We now introduce weights associated with each of these two operations, denoted
  $\theta_\uparrow$ and $\theta_\downarrow$, for each pair of words participating
  in a WordNet edge.
The score of a path is then defined as the dot product of the weights and features
  as described above: $\theta^{\textrm{T}}\phi$.

We can factorize this along the path
  $w_1, w_1^{(1)}, w_1^{(2)}, \dots, \textrm{lcs}, \dots, w_2^{(2)},  w_2^{(1)}, w_2$
  as follows:

\begin{align*}
\theta^{\textrm{T}}\phi
  &= \theta_\uparrow \left( 
    \left[\log p(w_1^{(1)}) - \log p(w_1)\right] +
    \dots +
    \left[\log p(w_1^{(n)}) - \log p(\textrm{lcs})\right]
    \right) + \\
  &~~ \theta_\downarrow \left( 
    \left[\log p(\textrm{lcs}) - \log p(w_1^{(n)}) \right] +
    \dots +
    \left[\log p(w_1) - \log p(w_1^{(1)})\right]
    \right) \\
  &= \theta_\uparrow \left( \log \frac{p(\textrm{lcs}}{p(w_1)} \right) +
     \theta_\downarrow \left( \log \frac{p(\textrm{lcs}}{p(w_2)} \right) \\
  &= \log \frac{ p(\textrm{lcs})^{\theta_\uparrow + \theta_\downarrow} }
               { p(w_1)^{\theta_\uparrow} + p(w_2)^{\theta_\downarrow} }
\end{align*}

Note that setting both $\theta_\uparrow$ and $\theta_\downarrow$ to 1 exactly
  yield JC similarity.



% --------------------
% NATURAL LOGIC
% --------------------
\Section{natlog}{Natural Logic}
Much of the underpinnings of this work rely on the theoretical framework
  laid by Natural Logic
  \cite{key:1986benthem-natlog,key:1991valencia-natlog}.
\todo{motivate NatLog}

An interpretation of Natural Logic as syllogistic reasoning,
  presented in \refsec{natlog-syllogism}, is sufficient
  for a working understanding of the contributions of this paper.
However, Natural Logic is both more powerful than syllogistic reasoning,
  and is more general than structural matching to known
  syllogistic templates.
\refsec{natlog-mono} introduces \textit{monotonicity} in the context
  of natural language to form the basis of natural logic;
  \refsec{natlog-exclusion} extends this formalism for reasoning about
  a wider range of phenomena.
\citet{key:2014icard-natlog} offers a good introduction and summary of
  Natural Logic, and provides a source for much of the notation and
  exposition presented here.

\Subsection{natlog-syllogism}{Syllogistic Reasoning}
\todo{write me}

\Subsection{natlog-mono}{Monotonicity in Natural Logic}
Monotonicity in Natural Logic derives its name from and correlates
  precisely to monotonicity of functions.
More precisely, a function can be one of \textit{monotone},
  \textit{antitone}, or \textit{non-monotone} in each of its
  arguments.\footnote{
    \textit{Monotone} and \textit{antitone} are often referenced
    as \textit{upwards monotone} and \textit{downwards monotone}.
  }
Intuitively, the function $f(x,y,z) = x - y + (z-2)^2$ is
  monotone in $x$, antitone in $y$, and non-monotone in $z$.
This fact, as well as the ordering of real numbers, allows us to
  draw inferences over the formula for $f$ without evaluating
  any part of the function explicitly.
For instance, we can safely say that $f(1,1,1) \leq f(2,1,1)$, whereas
  $f(1,1,1) \geq f(1,2,1)$.

Formally, we define a function $f:\sD_1 \rightarrow \sD_2$ and a partial
  ordering $\le_1$ over $\sD_1$ and $\le_2$ over $\sD_2$.
If we set $\sD_1 = \sD2 = \sR$, and take $\le_1 = \le_2 = \le$ to be
  the conventional ``less than or equal to'' over real numbers,
  we can derive the functions in our example above.
For instance, $f(x) = x + c$ is monotone; $g(x) = -x$ is antitone;
  $h(x) = (x-2)^2$ is non-monotone.

However, we can equally well define a partial ordering over sets:
  $x \le_s y \Leftrightarrow x \cap y = x$.
Similarly, we can define a partial ordering over functions
  $f : \sD_1 \rightarrow \sD_2$:
  $f \le_1 g \Leftrightarrow \forall x ~~ f(x) \leq_2 g(x)$.
This allows us to adapt our definition of monotonicity to quantifiers
  in natural language.

Let the denotation of a phrase, marked \denote{\cdot}, be the set of
  entities to which that phrase refers.
Equivalently, we may consider each phrase as a predicate, and the denotation
  of a phrase is the set of entities for which the predicate holds.
We define the set $e$ to be the set of entities in our discourse space,
  with a partial ordering $\le_e$ following the ordering over sets
  above: for every $x,y \subseteq e$, $x \le_e y \Leftrightarrow x \cap y = x$.
We define the set $t$ to be the set of truth values: $\{T, F\}$.
The ordering over truth values is trivially
  $x \le_t y \Leftrightarrow x = F \lor y = T$.
It should not go unnoticed that $x \Rightarrow y$ and $x \le_t y$ 
  are equivalent.
Note that functions $e \rightarrow t$ follow the partial ordering from
  above.

We can now define the denotation of quantifiers in the conventional
  fashion as functions $e \rightarrow (e \rightarrow t)$.
However, we can further mark the arguments to these quantifiers as
  potentially monotone or antitone.
For instance, the quantifier \textit{all} is antitone in its first
  argument and monotone in its second:
  $e \xrightarrow{-} (e \xrightarrow{+} t)$.\footnote{
    These markings can, in fact, be elegantly incorporated into the
    type system. Since this work does not make use of categorical
    grammars requiring strict interpretations of types, we refer the
    reader to \citet{key:2014icard-natlog} and do not elablorate
    on this topic further.
  }
This matches our intuition that, e.g., \textit{all cats have tails}
  implies that \textit{all tabby cats have tails}.

Formally, we can produce proofs of these sorts of facts by composing
  function application from atomic facts.
Returning to the arithmetic cast in the beginning of the section, we
  can construct a proof that $2^{-4} < 2^{-3}$ via:

\begin{prooftree}
\AxiomC{$3 < 4$}
\RightLabel{\scriptsize{$-x$ is antitone}}
\UnaryInfC{$-3 > -4$}
\RightLabel{\scriptsize{$2^x$ is monotone}}
\UnaryInfC{$2^{-3} > 2^{-4}$}
\end{prooftree}

In a similar vein, we can provide a proof for \textit{all tabby cats
  have tails} via:

\begin{prooftree}
\AxiomC{\denoteT{tabby cats} $\le_e$ \denoteT{cats}}
%\RightLabel{\scriptsize{\denoteT{All} is antitone in its first argument}}
\UnaryInfC{$
  \denoteT{all}\left(\denoteT{tabby cats}\right)\left(\cdot\right)
  \ge_{e \rightarrow t}
  \denoteT{all}\left(\denoteT{cats}\right)\left(\cdot\right)
$}
\AxiomC{\denoteT{have tails} $\le$ \denoteT{have tails}}
%\RightLabel{\scriptsize{$2^x$ is monotone}}
\BinaryInfC{$
  \denoteT{all}\left(\denoteT{tabby cats}\right)\left(\denoteT{have tails}\right)
  \ge_{t}
  \denoteT{all}\left(\denoteT{cats}\right)\left(\denoteT{have tails}\right)
$}
\end{prooftree}

That is to say, on a given parse of the respective sentences (in this
  case unambiguous),
  \denoteT{All cats have tails} $\leq_t$ \denoteT{All tabby cats have tails};
  and, as mentioned above, $\leq_t$ and modus ponens ($\Rightarrow$)
  are defined equivalently.


\Subsection{natlog-exclusion}{Reasoning with Exclusion}
Work by \citet{key:2009maccartney-natlog}, building off of
  \citet{2007maccartney-natlog} and
  \citet{key:2008maccartney-natlog},
  has focused on extending the set of atomic relationships between
  denotations of objects beyond $\leq_e$ (subset) and $\geq_e$ (superset).
We adopt the semantics of \citet{key:2012icard-natlog}, although in this
  work for simplicity we do not make use of their extended
  monotonicity markings.
A more thorough treatment of the soundness and completeness of the
  proof theory described here can be found in
  \citet{key:2013djalali-natlog}.

\citet{key:2014icard-natlog} noted that the partial ordering of
  $\leq_e$ can be formulated as a \textit{bounded distributive
  lattice} to encompass the original relations of
  \citet{key:2007maccartney-natlog}.
In particular, for every domain we can define a set of possible 
  elements $\sX$, binary operators $\land$ and $\lor$,
  and a minimum and maximum element $\bot$ and $\top$ respectively.
By example, for the domain of entities we can define $\sX$ to be
  the power set of our domain of entities, $\land=\cap$,
  $\lor=\cup$, $\bot=\{\}$, and $\top=E$, where $E$ is the universe
  of possible entities.
We then define the set of MacCartney relations, noting that
  $\forward$ and $\reverse$ are identical to the definitions of
  $\lte_e$ and $\gte_e$ from the previous section respectively:

\begin{center}
\begin{tabular}{lcl}
  $x \forward y$ & $\Leftrightarrow$ & $x \land y = x$ \\
  $x \reverse y$ & $\Leftrightarrow$ & $x \lor y = x$ \\
  $x \alternate y$ & $\Leftrightarrow$ & $x \land y = \bot$ \\
  $x \cover y$ & $\Leftrightarrow$ & $x \lor y = \top$ \\
  \vspace{-0.5em} & & \\
  $x \equivalent y$ & $\Leftrightarrow$ & $x \forward y~~\textrm{and}~~x \reverse y$ \\
  $x \negate y$ & $\Leftrightarrow$ & $x \alternate y~~\textrm{and}~~x \cover y$ \\
  $x \independent y$ & & Always true
\end{tabular}
\end{center}

Note that unlike in the original MacCartney formulation, these relations
  are not mutually exclusive; in fact, a hierarchy of informativeness
  emerges where, e.g., \negate subsumes \alternate and \cover.

We are now faced with two issues which need resolving to generalize
  the proofs from \refsec{natlog-mono}.
First, we need to resolve how the monotonicity of the function application
  projects the relation in the argument.
Implicitly used in the proofs so far is the notion that antitone contexts
  will flip $\leq$ and $\geq$, whereas monotone contexts will not.
This relation is preserved for $\forward$ and $\reverse$.
\citet{key:2012icard-natlog} present an enriched tagging of 
  monotonicity which allows the other relations (i.e.,
  \negateS, \alternateS, \coverS) to project meaningfully.
This work does not make use of this semantics, although it is not
  theoretically impossible to do so -- practically, this prohibits
  our system from reasoning about antonyms.

The second issue is determining how these relations project through
  the proof tree.

\bibliographystyle{authordate1}
\bibliography{../ref}

\end{document}



