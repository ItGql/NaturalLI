\Section{results}{Evaluation}
%We evaluate our system in two settings:
%  we sanity check that our modifications to NaturalLI have not hindered
%  its ability to capture strict logical entailments on the FraCaS test suite,
%  and then evaluate the system on answering multiple choice science questions
%  from the Aristo dataset.
%
%%
%% FraCaS
%%
%\Subsection{fracas}{FraCaS}
%\def\a#1{\textbf{#1}}
%\begin{table}
%\begin{center}
%  \resizebox{0.48\textwidth}{!}{
%  \begin{tabular}{llcccccccc}
%    \hline
%    $\mathsection$ & Category & Count & \multicolumn{2}{c}{Precision} & \multicolumn{2}{c}{Recall} & \multicolumn{3}{c}{Accuracy} \\
%                   &          &       & \blue{N} & M08          & \blue{N} & M08          & \blue{N}  & M08 & A14 \\
%    \hline
%                          % Pme           P07   Rme          R08   Ame         A07  A08
%    \a{1} & \a{Quantifiers}  & \a{44} & \a{\blue{ }}  & \a{95}  & \a{\blue{   }} & \a{100} & \a{\blue{}} & \a{97} & \a{95} \\
%    \a{5} & \a{Adjectives}   & \a{15} & \a{\blue{ }}  & \a{71}  & \a{\blue{   }} & \a{83}  & \a{\blue{}} & \a{80} & \a{73} \\
%    \a{6} & \a{Comparatives} & \a{16} & \a{\blue{ }}  & \a{88}  & \a{\blue{   }} & \a{89}  & \a{\blue{}} & \a{81} & \a{87} \\
%    \hline                                                                                                                 
%\multicolumn{2}{l}{\a{Total}}                                                                                              
%                         & \a{75}     & \a{\blue{ }}  & \a{89}  & \a{\blue{   }}  & \a{94}  & \a{\blue{}} & \a{90} & \a{89} \\
%    \hline
%  \end{tabular}
%  }
%  \caption{
%    \label{tab:fracas}
%    Results on the FraCaS textual entailment suite.
%    N is this work; M08 is \newcite{key:2008maccartney-natlog};
%    A14 is the original NaturalLI implementation in
%      \newcite{key:2014angeli-naturalli}.
%    Note that these results assume gold parse trees, and
%      that following prior work this is not a blind test set.
%  }
%\end{center}
%\end{table}
%
%% Intro to FraCaS
%The FraCaS corpus \cite{key:1996cooper-fracas}
%  is a small corpus of entailment problems, aimed at providing a
%  comprehensive test of a system's handling of various
%  entailment patterns.
%We process the corpus following \newcite{key:2007maccartney-natlog} and
%  \newcite{key:2014angeli-naturalli}.
%Results are reported in \reftab{fracas}.
%
%% Qualify results
%The results confirm that moving to dependency trees has not hindered the
%  system's ability to capture valid entailments.
%Although, it should be noted that the evaluation assumes gold dependency
%  trees, and following prior work the test suite is not a blind test
%  set.

%
% Aristo
%
% What is Aristo?
We evaluate our entailment system on the Regents Science Exam portion of
  the Aristo dataset \cite{key:2013clark-aristo,key:2015clark-aristo}.
The dataset consists of a collection of multiple-choice science questions
  from the New York Regents 4$^{\textrm{th}}$ Grade Science Exams
  \cite{key:NYSED}.
This is then cast as an question answering task -- given a corpus of
  supporting facts, find evidence for the truth of a particular answer.

% Why do we use it?
Our system is in many ways well-suited to the dataset.
Although certainly many of the facts require complex reasoning
  (see \refsec{aristo-analysis}), the majority of questions can be
  answered from a single premise -- allowing Natural Logic to be used
  as the inference formalism.
Unlike FraCaS or the RTE challenges, however, the task does not have explicit
  premises to run inference from, but rather must infer the truth of the
  hypothesis from a large collection of supporting text.
In contrast to the common-sense reasoning task in \newcite{key:2014angeli-naturalli}
  though, the queries in the Aristo dataset are relatively longer, complete sentences.
This necessitates the adaptations in \refsec{naturalli}, and benefits more from the
  soft signals from \refsec{softsignal}.

\Subsection{aristo-data}{Data Processing}
% Corpora
We make use of two collections of corpora for our experiments.
The first of these is the Barron's study guide (\textsc{Barron's}, 
  consisting of \num{1200} sentences.
This is the corpus used by \newcite{key:2015hixon-aristo} for their conversational
  dialog engine \knowbot, and therefore constitutes a more fair comparison against 
  their results.
However, we also make use of the full \textsc{Scitext} corpus \cite{key:2014clark-aristo}.
This corpus consists of \num{1316278} supporting sentences, 
  including the Barron's study guide alongside 
  simple Wikipedia, dictionaries, and a science textbook.

% Pre-processing
Since we lose all document context when searching over the corpus with NaturalLI,
  we first pre-process the corpus to resolve high-precision cases of
  pronominal coreference, via a set of simple high-precision sieves.
The corpus is then filtered for non-ascii characters (yielding \num{945956} sentences),
  and exact duplicate sentences are removed.
This results in a total of \num{822748} facts in the supporting corpus.

% Lucene
These sentences were then indexed using Solr.
The set of promising premises for the soft alignment in \refsec{softsignal}, as well as
  the Lucene score feature in the lexical classifier (\refsec{softsignal-classifier}),
  were obtained by querying Solr using the default similarity metric and scoring function.

% Questions to Candidates
On the query side, questions were converted to answers using the same methodology as
  \newcite{key:2015hixon-aristo}.
In cases where the question contained multiple sentences, only the last sentence
  was considered.
As discussed in \refsec{aristo-analysis}, 
  this renders these multi-sentence questions effectively out of scope for the model.

%
% Mutliple Choice as Entailment
%
\Subsection{aristo-train}{Training an Entailment Classifier}
To train a soft entailment classifier, we needed a training set of positive
  and negative entailment instances.
These were collected on mechanical turk from a set of candidate entailment pairs.
In particular, for each true hypothesis in the training set and for each sentence
  in the Barron's study guide, we found the top 8 results from Lucene and considered
  these to be candidate entailments.
These were then shown to Turkers, who decided whether the premise entailed the
  hypothesis, the hypothesis entailed the premise, both, or neither.
Note that each pair was shown to only one Turker, lowering the cost of
  data collection, but consequently resulting in a somewhat noisy dataset.

The data was then augmented with an additional set of negatives, collected by taking
  the top 10 Lucene results for each negative hypothesis in the training set.
This yielded a total of \num{21306} training examples, which was used to train
  a simple logistic regression classifier.

As a simple heuristic for incorporating soft negation, the classifier's overlap
  score was halved in these cases.
Additionally, any premise which did not contain the candidate answer to the
  multiple choice query was discounted by three quarters (if it is a single
  word answer) and a quarter (if it is a multi-word answer).
Note that despite these, the system is relatively domain agnostic -- no
  lexical features or domain-specific reasoning is employed.

%
% Results
%
\Subsection{aristo-results}{Experimental Results}
Results on the Aristo dataset are reported in \reftab{aristonaturalli}.
\textit{NaturalLI} denotes our system, using NaturalLI inferences incorporating
  the soft classifier.
%Note that this already performs as well as prior work published on the dataset,
%  and scales gracefully to a larger supporting corpus.
%
We additionally report results for a number of traditional baselines.
The simplest of these is to rank answers based on the Lucene score returned by
  Solr.
This turned out to be a surprisingly strong baseline -- furthermore, as the supporting
  corpus size increases, the scores become significantly better calibrated, and
  performance improves dramatically.

Our second baseline uses only the keyword overlap classifier, either with or without
  the Lucene scores included as a feature.
Both of these baselines offer a boost over using the Lucene score alone -- although
  it's interesting to note that the Lucene score actively hurts the classifier's
  performance if the corpus size is small (i.e, when run on the Barron's corpus).

Lastly, we present results for the combination of NaturalLI with a soft classifier,
  and show that it outperforms both prior work and the above baselines.
In fact, on the training set, the classifier can nearly pass the exam with a score
  of \todo{hopefully 70?}.

%
% ARISTO NATURALLI ONLY
%
% Some definitions
\def\t#1{\small{#1}}
\def\b#1{\t{\textbf{#1}}}
\def\colspaceS{2.0mm}
\def\colspaceM{3.0mm}
\def\colspaceL{4.0mm}

% The table
\begin{table}
\begin{center}
\begin{tabular}{l@{\hskip \colspaceL}c@{\hskip \colspaceS}c@{\hskip \colspaceL}c@{\hskip \colspaceS}c}
\hline
\textbf{System} & \multicolumn{2}{c}{\textbf{Barron's}} & \multicolumn{2}{c}{\textbf{\textsc{Scitext}}} \\
 & Train & Test & Train & Test \\
\hline
\t{\textsc{Knowbot} (held-out)} & \t{45} & \t{--} & \t{--} & \t{--} \\
\t{\textsc{Knowbot} (oracle)}   & \t{57} & \t{--} & \t{--} & \t{--} \\
\hline                                                         
\t{NaturalLI}                   & \t{51?} & \t{  } & \t{55?} & \t{ } \\
\t{Solr Only}                   & \t{48?} & \t{  } & \t{64?} & \t{ } \\
\t{Align Features Only}         & \t{54?} & \t{  } & \t{67.6?} & \t{ } \\
\t{Align + Solr}                & \t{50?} & \t{  } & \t{68.5?} & \t{ } \\
\t{Align + Solr + NaturalLI}    & \b{50?} & \b{  } & \b{70.4?} & \b{ } \\
\hline
\end{tabular}
\end{center}
% The caption
\caption{
\label{tab:aristonaturalli}
Accuracy of various systems on the Aristo science questions dataset.
Results are reported using only the Barron's study guide as the supporting
  text, and using all of \textsc{Scitext}.
\textsc{Knowbot} is the dialog system presented in \newcite{key:2015hixon-aristo}.
The held-out version used only additional facts from other question's dialogs;
  the oracle version directly made use of human input on the question it was 
  answering.
The test set did not exist at the time \textsc{Knowbot} was published.
}
\end{table}
%
% END ARISTO NATURALLI ONLY
%

\Subsection{aristo-analysis}{Discussion}
We analyze some common types of errors made by the system on the training set.
The most common error can be attributed to the question requiring complex reasoning
  about multiple premises.
\num{29} of \num{108} questions in the training set (26\%) contain multiple
  premises.
Although there is usually still some signal for which answer is most likely to be correct,
  these questions are fundamentally out-of-scope for the approach.
Empirically, we correctly predict \num{XYZ} of these questions, performing somewhat 
  but not substantially above random chance.

Other questions are simply not supported by any single sentence in the corpus.
For example, the question \w{A human offspring can inherit blue eyes.} has
  no support in the corpus that does not require significant multi-step inferences.
A remaining chunk of errors is, of course, caused by errors in the soft lexical classifier.
