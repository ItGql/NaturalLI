\Section{bg}{Background}
We briefly review natural logic and the existing NaturalLI system.
Much of this paper extends this system, with additional inferences
  (\refsec{naturalli}) and a soft lexical classifier (\refsec{softsignal}).

\Subsection{bg-natlog}{Natural Logic}
% Bag of citations
Natural logic is a formal proof theory that 
  aims to capture a subset of valid logical
  inferences by appealing directly to the structure of language,
  without needing either an abstract logical language 
  (e.g., Markov Logic Networks; \newcite{key:2006richardson-mln})
  or denotations (e.g., semantic parsing; \newcite{key:2015liang-denotations}).
Like \newcite{key:2014angeli-naturalli}, we use the logic
  introduced by the NatLog system (MacCartney and Manning, 2007; 2008; 2009),
  \nocite{key:2007maccartney-natlog}
  \nocite{key:2008maccartney-natlog}
  \nocite{key:2009maccartney-natlog},
  which was in turn
  based on earlier theoretical work on Monotonicity Calculus
  \cite{key:1986benthem-natlog,key:1991valencia-natlog}.
We adopt the precise semantics of \newcite{key:2014icard-natlog};
  we refer the reader to this paper for a more thorough introduction to
  the formalism.

% Give a teaser
At a high level, natural logic proofs operate by mutating spans of text
  to ensure that the mutated sentence follows from the
  original -- each step is much like a syllogistic inference.
Each mutation in the proof follows three steps:

\begin{enumerate}
\setlength\itemsep{-0.25em}
\item An atomic lexical relation is induced by either inserting, deleting
      or mutating a token in the sentence. 
      For example, in \reffig{naturalli},
      mutating \w{The} to \w{No} induces the \negate\ relation;
      mutating \w{cat} to \w{carnivore} induces the \forward\ relation.
      
      The relations \equivalent\ and \forward\ are variants of  entailment;
      \negate\ and \alternate\ are variants of negation.

\item This lexical relation between words is projected up to yield a relation between
      sentences, based on the operators in the sentence.
      For instance, \w{The cat eats animals} \forward\ \w{the carnivores eat animals}.
      We explain this in more details below.

\item These sentence level relations are \textit{joined} together to produce a
      relation between a premise, and a hypothesis multiple mutations away.
      For example in \reffig{naturalli}, if we join 
      \forward, \equivalent, \forward, and \negate, we
      get negation (\alternate).
\end{enumerate}

%Some example relations are given below, corresponding to the example inference
%  in \reffig{naturalli}:
%
%\vspace{1.0em}
%\begin{center}
%\begin{tabular}{rcl}
%\w{No $x$ $y$} & \negate     & \w{The $x$ $y$} \\
%\w{cat}        & \forward    & \w{carnivore} \\
%\w{animal}     & \equivalent & \w{a animal} \\
%\w{animal}     & \reverse    & \w{mouse} \\
%\end{tabular}
%\end{center}
%\vspace{1.0em}
%
%% Punt on things
%We refer the reader to \newcite{key:2014icard-natlog} for a
%  more comprehensive introduction to the atomic relations (1), and to the
%  join table between these relations (3).

The notion of \textit{projecting} a relation from a lexical item to
  a sentence is important to understand.\footnote{
    For clarity we describe a simplified semantics here; 
    NaturalLI implments the semantics descibed in 
    \newcite{key:2014icard-natlog}.
  }
To illustrate, \textit{cat} \forward\ \textit{animal},
  and \textit{some cat meows} \forward\ \textit{some animal meows}
  (recall, \forward\ denotes entailment),
  but
  \textit{no cat barks} \nforward\ \textit{no animal barks}.
Despite differing by the same \textit{lexical} relation,
  the \textit{sentence-level} relation is different in the two cases.

% Monotonicity
We appeal to two important concepts: \textit{monotonicity} -- a
  property of arguments to natural language operators;
  and \textit{polarity} -- a property of tokens.
From the example above, \w{some} is [upward] monotone in its first
  argument (i.e., \w{cat} or \w{animal}), 
  and \w{no} is antitone (downward monotone) in its first argument.
This means that the first argument to \w{some} is allowed to mutate up the
  hypernymy tree, whereas the first argument to \w{no} is allowed to mutate
  down the hypernymy tree.

% Polarity
\textit{Polarity} is
  a property of tokens in a sentence determined by the
  operators acting on it.
All lexical items have \textit{upward} polarity by default;
  monotone operators -- like \w{some}, \w{several}, or \w{a few} -- preserve polarity.
Antitone operators -- like \w{no}, \w{not}, \w{all} in its first argument -- 
  reverse polarity.
For example, \w{mice} in \w{\textbf{no} cats eat mice} has downward 
  polarity, whereas \w{mice} in \w{\textbf{no} cats do\textbf{n't} eat mice}
  has upward polarity
  (it is in the scope of two downward monotone operators).
The polarity of a token defines how a relation induced by a lexical mutation
  projects up a sentence, and therefore completely determines the valid mutations
  on that token.

%
% NaturalLI
%
\Subsection{bg-natlog}{NaturalLI}

% Teaser search figure
\begin{figure}[t]
\begin{center}
  \resizebox{0.48\textwidth}{!}{\teaserSearch} \\
\end{center}
\caption{
  An illustration of a NaturalLI searching for a candidate premise
  to support the hypothesis at the root of the tree.
  We are searching from a hypothesis
  \w{no carnivores eat animals}, and find a contradicting
  premise \w{the cat ate a mouse}.
  The edge labels denote Natural Logic inference steps.
  \label{fig:naturalli}
}
\end{figure}

% Introduce NaturalLI
This paper extends NaturalLI, presented in \newcite{key:2014angeli-naturalli}.
NaturalLI casts inference as a search problem: given a hypothesis and an arbitrarily
  large knowledge base of candidate premises, it searches through the space of
  lexical mutations (e.g., \w{cat} $\rightarrow$ \w{carnivore}), 
  with associated costs, until a premise is found.
This allows it to capture both strict natural logic inferences (e.g., over hypernymy
  or operator mutations), as well as likely correct inferences with an associated
  cost (e.g., nearest neighbors in vector space).
Most importantly for a question answering task, however, NaturalLI allows for an
  arbitrarily large knowledge base to search for answers in, contrasted with traditional
  entailment systems which classify entailment between one (or a few) premises and a
  hypothesis.

% Explain example
An example search using NaturalLI is given in \reffig{naturalli}.
The relations along the edges denote relations between the associated sentences
  -- i.e., the projected lexical relations from \refsec{bg-natlog}.
In practice, NaturalLI generally searches around \num{100000} candidate premises
  per second, on knowledge bases ranging from 1M premises (the \textsc{Scitest}
  corpus used in this work) to 300M premises (the dataset used in the original paper).

%
% Related Work
%
\Subsection{bg-qa}{Related Work}
\todo{don't plagiarize the EMNLP2014 paper}
\todo{also, should be shorter...}
% -- Applications
Many NLP applications query large knowledge bases.
Prominent examples include
  question answering
    \cite{key:2001voorhees-trec},
  semantic parsing
    \cite{key:1996zelle-semantics,key:2007zettlemoyer-semantics,key:2013kwiatkowski-semantics,key:2014berant-semantics},
  and information extraction systems
    \cite{key:2011hoffman-kbp,key:2012surdeanu-mimlre}.
A goal of this work is to improve accuracy on these
  downstream tasks by providing a \textit{probabilistic} knowledge base
  for
%  both explicitly known and 
  likely true facts.

% -- Extending OpenIE
% (people extending OpenIE)
A natural alternative to the approach taken in this paper is to
  extend knowledge bases by inferring and adding new facts directly.
% (misc)
For instance,
  \newcite{key:2006snow-wordnet} present an approach to enriching 
    the WordNet taxonomy;
  \newcite{key:2011tandon-conceptnet} extend ConceptNet with new facts;
  \newcite{key:2010soderland-adapting} use ReVerb extractions to 
    enrich a domain-specific ontology.
% Richard
\newcite{key:2013chen-completion} and \newcite{key:2013socher-completion}
  use Neural Tensor Networks to predict unseen relation triples in
  WordNet and Freebase, following a line of work by
  \newcite{key:2011bordes-completion} and
  \newcite{key:2012jenatton-completion}.
% Universal schemas
\newcite{key:2012yao-schemas} and \newcite{key:2013riedel-schemas}
  present a related line of work, inferring new relations between
  Freebase entities via inference over both Freebase and
  OpenIE relations.
In contrast, this work runs inference over arbitrary text, without 
  restricting itself to a particular set of relations, or even entities.
%This work, however, focuses primarily on common-sense reasoning rather
%  than inferring relations between named entities.

% -- RTE
This work is similar in many ways to work on 
  recognizing textual entailment -- e.g., 
  \newcite{key:2010-schoenmackers-horn}, \newcite{key:2011berant-entailment}.
Work by \newcite{key:2013lewis-entailment} is particularly relevant,
  as they likewise evaluate on the FraCaS suite (Section 1;
  89\% accuracy with gold trees).
They approach entailment by constructing a CCG parse of the query,
  while mapping questions which are paraphrases of each other to the
  same logical form using distributional relation clustering.
However, their system is unlikely to scale to either our large
  database of premises, or our breadth of relations.

% Paraphrase-based Q/A
\newcite{key:2014fader-openqa} propose a system for question answering
  based on a sequence of paraphrase rewrites followed by a fuzzy query to
  a structured knowledge base.
This work can be thought of as an elegant framework for unifying this
  two-stage process, while explicitly tracking the ``risk'' taken with
  each paraphrase step.
Furthermore, our system is able to explore mutations which are only
  valid in one direction, rather than the bidirectional entailment of
  paraphrases, and does not require a corpus of such paraphrases for
  training.



