\documentclass[hyperref]{beamer}
\usepackage{beamerthemesplit}
\usepackage{graphicx}
\usepackage{mathptmx}           % replacement for obsolete \usepackage{times}
\usepackage[scaled=1.0]{helvet} % replacement for obsolete \usepackage{times}
\usepackage{courier}            % replacement for obsolete \usepackage{times}
\usepackage[normalem]{ulem}

\usepackage{tikz}
\usetikzlibrary{shapes.arrows,chains,positioning,automata,trees,calc}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.pathmorphing,decorations.markings}
\usepackage{times,latexsym,amsfonts,amssymb,amsmath,graphicx,url,bbm,rotating,siunitx}
\usepackage{multirow,hhline,arydshln,array,color,stmaryrd,pifont}
\definecolor{darkred}{rgb}{0.5, 0.0, 0.0}
\definecolor{darkgreen}{rgb}{0.0, 0.4, 0.0}
\definecolor{darkblue}{rgb}{0.0, 0.0, 0.5}

% set up Beamer style with Stanford colors and logo
% logo is available at http://nlp.stanford.edu/local/nlp-logos/nlp-logo.pdf
\useinnertheme{rounded}
\useoutertheme{infolines}
\usecolortheme{beaver}
\setbeamercolor{block title}{fg=white,bg=darkred!75!black}
\setbeamercolor{block body}{parent=normal text,bg=black!5!bg}
\setbeamercolor{item projected}{bg=darkred}
\logo{\includegraphics[height=1cm]{../../img/nlp-logo.pdf}}

% title page information
\title{NaturalLI: Natural Logic Inference for Common Sense Reasoning}
\subtitle{}
\author{Gabor Angeli, Chris Manning}
\date{October 42, 2014}
\institute[Stanford]{Stanford University}

\input ../../macros.tex
\input ../../figures.tex

\begin{document}
\begin{frame}[noframenumbering]
  \titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MOTIVATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% 
% COMMON SENSE REASONING TEASER
%%%%%%%%%%%%%%%%%%%
\begin{frame}{\scalebox{0.95}{Natural Logic Inference for \uline{\textbf{Common Sense Reasoning}}}}
\begin{tabular}{cc}
  \green{Kittens play with yarn} & \red{Kittens play with computers} \\
  \vspace{0.25cm} \\
  \includegraphics[width=5cm]{../../img/yarn-cat.jpg} & \pause \includegraphics[width=5cm]{../../img/computer-cat-cropped.jpg}
\end{tabular}
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% COMMON SENSE REASONING DEFN
%%%%%%%%%%%%%%%%%%%
\begin{frame}{\scalebox{0.95}{Natural Logic Inference for \uline{\textbf{Common Sense Reasoning}}}}
\begin{center}
\begin{tabular}{ll}
  \hh{Input}  & A query fact (e.g., \w{cats have tails}). \\
  & \\
  \hh{Output} & \textbf{True} or \textbf{False}, with some confidence.
\end{tabular}
\end{center}
\pause

\vspace{0.5cm}
\hh{Subtleties we ignore:}
\begin{itemize}
  \item \textbf{Truth is elusive.}
    \begin{itemize}
      \item \true{birds fly} but \false{penguins don't fly}.
      \pause
      \item \unknown{Obama is a good president}.
      \pause
      \item A fact is true if we would accept it without evidence to the contrary.
    \end{itemize}
  \pause
  \item \textbf{The internet lies.}
    \begin{itemize}
      \item \false{Obama was born in Kenya}.
    \end{itemize}
  \pause
  \item \textbf{This query is false.}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% COMMON SENSE REASONING NLP
%%%%%%%%%%%%%%%%%%%
\begin{frame}{\uline{\textbf{Common Sense Reasoning}} for NLP}
\begin{center}
  \w{The city refused the demonstrators a permit because they feared violence.} \\
  \pause
  \begin{tabular}{l}
    \true{a city fears violence} \\
    \false{demonstrators fear violence}
  \end{tabular}
\end{center}
\pause

\begin{center}
  \w{I ate the cake with a cherry} \hspace{0.25cm} vs. \hspace{0.25cm} \w{I ate the cake with a fork} \\
  \begin{tabular}{l}
    \true{cakes come with cherries} \\
    \false{cakes are eaten using cherries}
  \end{tabular}
\end{center}
\pause

\begin{center}
  \w{Put a sarcastic comment in your talk. That's a great idea.} \\
  \begin{tabular}{l}
    \false{Sarcastic comments are a great idea in talks.}
  \end{tabular}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%% 
% COMMON SENSE REASONING VISION
%%%%%%%%%%%%%%%%%%%
\begin{frame}{\uline{\textbf{Common Sense Reasoning}} for Vision}
\begin{tabular}{cc}
  \red{Dogs drive cars} & \green{People drive cars} \\
  \includegraphics[height=3cm]{../../img/dog-driving.jpg} & \includegraphics[height=3cm]{../../img/person-driving.jpg}  \\
  \vspace{0.0cm} \\
  \pause \red{Baseball is played underwater} & \green{Baseball is played on grass} \\
  \includegraphics[height=3cm]{../../img/baseball-underwater.jpg} & \includegraphics[height=3cm]{../../img/baseball-grass.jpg} \\
\end{tabular}
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% TEASER
%%%%%%%%%%%%%%%%%%%
\begin{frame}{Start with a large knowledge base}
\begin{center}
  \teaserManyPremises
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{Infer new facts...}
\begin{center}
  \teaserBlindInferenceNaturalOrder
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{Infer new facts...on demand from a query...}
\begin{center}
  \teaserBlindInference
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{...Using text as the meaning representation...}
\begin{center}
  \teaserInference
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{...Without aligning to any particular premise.}
\begin{center}
  \teaserFullDerivation
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%% 
% INFERENCE MOTIVATION
%%%%%%%%%%%%%%%%%%%
\begin{frame}{\scalebox{0.95}{Natural Logic \uline{\textbf{Inference}} for Common Sense Reasoning}}
\begin{center}
  \teaserBlindInferenceNaturalOrder
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{\scalebox{0.95}{Natural Logic \uline{\textbf{Inference}} for Common Sense Reasoning}}
\hh{Lots of common sense knowledge bases:}
\begin{itemize}
  \item OpenIE (TextRunner, ReVerb, Ollie, OpenIE4).
  \item NELL, ConceptNet (OpenMind Common Sense), OpenCyc, etc.
\end{itemize}
\vspace{0.5cm}
\pause

\hh{A key challenge is coverage.}
\pause
\begin{itemize}
  \item We can greatly improve coverage with \textit{inference}.
\end{itemize}
\vspace{0.5cm}
\pause

\hh{Contrast: RTE challenges.}
\begin{itemize}
  \item \red{Few premises}; \green{many inference steps}.
\end{itemize}
\pause

\hh{Contrast: Relation inference.}
\begin{itemize}
  \item \green{Many premises}; \red{few inference steps}.
\end{itemize}
\end{frame}

\begin{frame}[noframenumbering]{\scalebox{0.95}{Natural Logic \uline{\textbf{Inference}} for Common Sense Reasoning}}
\begin{center}
  \teaserBlindInferenceNaturalOrder
\end{center}
\end{frame}
\begin{frame}[noframenumbering]{\scalebox{0.95}{Natural Logic \uline{\textbf{Inference}} for Common Sense Reasoning}}
\begin{center}
  \teaserBlindInference
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% NATLOG MOTIVATION
%%%%%%%%%%%%%%%%%%%
\begin{frame}{Properties of an inference algorithm}
\hh{Want to make many inference steps}
\begin{itemize}
  \item[$\Rightarrow$] Computationally fast during inference.
\end{itemize}
\pause
\vspace{0.5cm}

\hh{Want to leverage large knowledge base}
\begin{itemize}
  \item[$\Rightarrow$] Computationally fast during pre-processing.
\end{itemize}
\pause
\vspace{0.5cm}

\hh{Still want to capture common inferences}
\pause
\vspace{0.5cm}

\begin{center}
  \begin{tabular}{rl}
    \multicolumn{1}{m{3cm}}{\includegraphics[width=3cm]{../../img/superhero.pdf}} &
    \huge{Natural Logic}
  \end{tabular}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NATURAL LOGIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% 
% NATLOG AS SYLLOGISMS
%%%%%%%%%%%%%%%%%%%
\begin{frame}{\scalebox{0.95}{\uline{\textbf{Natural Logic}} Inference for Common Sense Reasoning}}
\begin{center}
  \teaserInference
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{Natural Logic as Syllogisms}
\begin{center}
  \hh{s/Natural Logic/Syllogistic Reasoning/g}
\end{center}
\vspace{0.25cm}
\pause

\hh{Quantifiers are important}
\vspace{0.25cm}
\hspace{0.5cm}
\begin{tabular}{lp{4cm}lp{5cm}}
  &\textbf{All} Greeks are mortal & & \textbf{Some} Greeks are women \\
  &Socrates is Greek & & Socrates is Greek \\
  $\therefore$& \true{Socrates is Mortal} & $\therefore$ & \false{Socrates is a woman} \\
\end{tabular}
\vspace{0.5cm}
\pause

\hh{Half the premises are in WordNet} \\
\vspace{0.25cm}
\hspace{0.5cm}
\begin{tabular}{lp{6cm}}
  &\textbf{Some} cat ate a mouse \\
  &\sout{Cats are carnivores} \\
  &\sout{Mice are animals} \\
  $\therefore$& \true{Some carnivore ate an animal} \\
\end{tabular}
\vspace{0.5cm}
\pause

\hh{Logic of \textit{lexical mutations} warranted by governing quantifiers.}
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% NATLOG ANIMATION
%%%%%%%%%%%%%%%%%%%
\input example.tex

%%%%%%%%%%%%%%%%%%% 
% BEYOND SYLLOGISMS
%%%%%%%%%%%%%%%%%%%
\input extensions.tex

%%%%%%%%%%%%%%%%%%% 
% ADVANTAGES OF NATLOG
%%%%%%%%%%%%%%%%%%%
\begin{frame}{Properties of Natural Logic}
\begin{itemize}
  \item[\green{\checkmark}] Computationally fast during inference.
  \begin{itemize}
    \item ``Semantic'' parse of query is just syntactic parse.
    \item Inference is lexical mutations / insertions / deletions.
  \end{itemize}
  \vspace{0.5cm}
  \pause

  \item[\green{\checkmark}] Computationally fast during pre-processing.
  \begin{itemize}
    \item Plain text!$^*$
    \pause
    \item[] ($^*$Generated from Ollie extractions.)
  \end{itemize}
  \vspace{0.5cm}
  \pause

  \item[\green{\checkmark}] Still captures common inferences.
  \begin{itemize}
    \item We make these types of inferences regularly and instantly.
    \pause
    \item We expect \textit{readers} to make these inferences instantly.
  \end{itemize}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INFERENCE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% 
% INFERENCE IS SEARCH
%%%%%%%%%%%%%%%%%%%
\begin{frame}{Natural Logic Inference is Search}
  \teaserFullDerivation
\end{frame}

\begin{frame}[noframenumbering]{Natural Logic Inference is Search}
\begin{center}
  \includegraphics[width=5cm]{../../img/dijkstras-graph.pdf}
\end{center}
\begin{tabular}{ll}
  \hh{Nodes} & $($ \w{fact}, validity $\in\{\textrm{valid}, \textrm{invalid}\})$ \\
  & \\
  \pause
  \hh{Start Node} & \w{query fact} \\
  \hh{End Nodes}  & \w{any known fact} \\
  & \\
  \pause
  \hh{Edges} & Mutations of the current fact \\
  \pause
  \hh{Edge Costs} & How ``wrong'' an inference step is (learned). \\
\end{tabular}
\end{frame}

%%%%%%%%%%%%%%%%%% 
% EXAMPLE SEARCH
%%%%%%%%%%%%%%%%%%
\input exampleSearch.tex

%%%%%%%%%%%%%%%%%%% 
% EDGE TEMPLATES
%%%%%%%%%%%%%%%%%%%
\begin{frame}{Edge Templates}
\begin{center}
  \begin{tabular}{p{0.4\textwidth}p{0.20\textwidth}}
    \multicolumn{1}{c}{\textbf{Template}} & \multicolumn{1}{c}{\textbf{Instance}} \\
    & \\
    Hypernym & \w{animal} $\rightarrow$ \w{cat} \\
    Hyponym  & \w{cat} $\rightarrow$ \w{animal} \\
    Antonym  & \w{good} $\rightarrow$ \w{bad} \\
    Synonym  & \w{cat} $\rightarrow$ \w{true cat} \\
    Add Word  & \w{cat} $\rightarrow$ \w{$\cdot$} \\
    Delete Word  & $\cdot$ $\rightarrow$ \w{cat} \\
    Quantifier Weaken & \w{some} $\rightarrow$ \w{all} \\
    Quantifier Strengthen & \w{all} $\rightarrow$ \w{some} \\
    Quantifier Negate & \w{all} $\rightarrow$ \w{no} \\
    Quantifier Synonym & \w{all} $\rightarrow$ \w{every} \\
    \pause \\
    Nearest Neighbor  & \w{cat} $\rightarrow$ \w{dog} \\
  \end{tabular}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%% 
% DELETIONS (INSERTIONS)
%%%%%%%%%%%%%%%%%%%
\begin{frame}{Inserting Words During Search}
\begin{center}
  \scalebox{0.8}{\wordthe \cat \eat \mice}
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{Inserting Words During Search}
\begin{center}
  \scalebox{0.8}{\wordthe \cat \eat \worda \mice}
\end{center}
\end{frame}

\def\worda{\monoUpR{}{???}{}{}}
\begin{frame}[noframenumbering]{Inserting Words During Search}
\begin{center}
  \scalebox{0.8}{\wordthe \cat \eat \worda \mice} \\
  \vspace{0.5cm}
  \exampleInsertions
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% STORE FACTS AS A TRIE
%%%%%%%%%%%%%%%%%%%
\def\title{Store Facts as a Trie}
\begin{frame}{\title}
  \factTrie{dotpath}{dotpath}{dotpath}
\end{frame}

\begin{frame}[noframenumbering]{\title}
  \factTrie{goodPath}{dotpath}{dotpath}
\end{frame}

\begin{frame}[noframenumbering]{Inserting Words During Search}
\begin{center}
  \scalebox{0.8}{\wordthe \cat \eat \worda \mice}
\end{center}
\end{frame}

\def\worda{\monoUpR{}{catnip}{herb}{tracheophyte}}
\begin{frame}[noframenumbering]{Inserting Words During Search}
\begin{center}
  \scalebox{0.8}{\wordthe \cat \eat \worda \mice}
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{\title}
  \factTrie{dotpath}{goodPath}{dotpath}
\end{frame}

\def\worda{\monoUpR{}{???}{}{}}
\begin{frame}[noframenumbering]{Inserting Words During Search}
\begin{center}
  \scalebox{0.8}{\wordthe \cat \eat \worda \mice}
\end{center}
\end{frame}

\def\worda{\monoUpR{}{my}{}{}}
\begin{frame}[noframenumbering]{Inserting Words During Search}
\begin{center}
  \scalebox{0.8}{\wordthe \cat \eat \worda \mice}
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{\title}
  \factTrie{dotpath}{dotpath}{goodPath}
\end{frame}

\def\worda{\monoUpR{}{???}{}{}}
\begin{frame}[noframenumbering]{Inserting Words During Search}
\begin{center}
  \scalebox{0.8}{\wordthe \cat \eat \worda \mice}
\end{center}
\end{frame}

\def\worda{\monoUpR{\textbf{All$_{\downarrow \uparrow}$}}{\darkgreen{\textbf{a$_{\uparrow \uparrow}$}}}{}{}}
\begin{frame}[noframenumbering]{Inserting Words During Search}
\begin{center}
  \scalebox{0.8}{\wordthe \cat \eat \worda \mice}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% CONTRIBUTION: COLLAPSED INFERENCE STATES
%%%%%%%%%%%%%%%%%%%
\def\title{Contribution: The Missing Slides}
\def\blurb{
\hh{Inference according to us:} The truth of a node is \textit{maintained} or
  \textit{flipped} based on the template of the outgoing edge}
\begin{frame}{\title}
\blurb.
\end{frame}

\begin{frame}[noframenumbering]{\title}
\blurb\ \darkgreen{\textbf{and the truth of the node}}.
\end{frame}

\begin{frame}[noframenumbering]{\title}
\blurb\ and the truth of the node 
  \darkgreen{\textbf{and the exact quantifiers in scope}}.
\end{frame}

\def\blurb{
\hh{Inference according to us:} The truth of a node is \textbf{\textit{maintained} or
  \textit{flipped}} based on the template of the outgoing edge and the truth of the node 
  and the exact quantifiers in scope.
}
\begin{frame}[noframenumbering]{\title}
\blurb
\end{frame}


\def\joinTable{
  \begin{tabular}{|c||c|c|c|c|c|c|c|}
    \hline
    $\bowtie$ & $\equivalent$ & $\forward$ & $\reverse$ & $\negate$ & $\alternate$ & $\cover$ & $\independent$ \\
    \hline
    $\equivalent$ & $\equivalent$ & $\forward$ & $\reverse$ & $\negate$ & $\alternate$ & $\cover$ & $\independent$ \\
    $\forward$ & $\forward$ & $\forward$ & $\independent$ & $\alternate$ & $\alternate$ & $\independent$ & $\independent$ \\
    $\reverse$ & $\reverse$ & $\independent$ & $\reverse$ & $\cover$ & $\independent$ & $\cover$ & $\independent$  \\
    $\negate$ & $\negate$ & $\cover$ & $\alternate$ & $\equivalent$ & $\reverse$ & $\forward$ & $\independent$  \\
    $\alternate$ & $\alternate$ & $\independent$ & $\alternate$ & $\forward$ & $\independent$ & $\forward$ & $\independent$  \\
    $\cover$ & $\cover$ & $\cover$ & $\independent$ & $\reverse$ & $\reverse$ & $\independent$ & $\independent$  \\
    $\independent$ & $\independent$ & $\independent$ & $\independent$ & $\independent$ & $\independent$ & $\independent$ & $\independent$ \\
    \hline
	\end{tabular}
}
\begin{frame}[noframenumbering]{\title}
\blurb \\
\begin{center}	
  \w{mammal} \reverse\ \w{cat} $~~\vdash~~$ \w{all mammals have fur} $\forward$ \w{all cats have fur} \\
  \pause
  \vspace{0.25cm}
  \joinTable
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{\title}
\blurb \\
\begin{center}	
  \uline{\w{mammal} \reverse\ \w{cat}} $~~\vdash~~$ \w{all mammals have fur} $\forward$ \w{all cats have fur} \\
  \vspace{0.25cm}
  \joinTable
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{\title}
\blurb \\
\begin{center}	
  \w{mammal} \reverse\ \w{cat} $~~\vdash~~$ \uline{\w{all mammals have fur} $\forward$ \w{all cats have fur}} \\
  \vspace{0.25cm}
  \joinTable
\end{center}
\end{frame}

\begin{frame}[noframenumbering]{\title}
\blurb \\
\begin{center}	
  \w{mammal} \reverse\ \w{cat} $~~\vdash~~$ \uline{\w{all mammals have fur} $\Rightarrow$ \w{all cats have fur}} \\
  \vspace{0.25cm}
  \joinTable
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LEARNING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% 
% SOFT LOGIC
%%%%%%%%%%%%%%%%%%%
\begin{frame}{``Soft'' Natural Logic}
\hh{Want to make likely (but not certain) inferences}.
\pause
\begin{itemize}
  \item Same motivation as Markov Logic, Probabilistic Soft Logic, etc.
  \pause
  \item Each \textit{edge template} has a cost $\theta \geq 0$.
\end{itemize}
\vspace{0.5cm}
\pause

\hh{Detail:} Variation among \textit{edge instances} of a template.
\begin{itemize}
  \item WordNet: \w{cat} $\rightarrow$ \w{feline} \textbf{vs.} \w{cup} $\rightarrow$ \w{container}.
  \item Nearest neighbors distance.
  \pause
  \item Each \textit{edge instance} has a distance $f$.
\end{itemize}
\vspace{0.5cm}
\pause

\begin{tabular}{ll}
\hh{Cost of an edge is} & $\theta_i \cdot f_i$. \\
\pause
\hh{Cost of a path is} & $\theta \cdot \mathbf{f}$. \\
\end{tabular}
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% Cost -> Probability
%%%%%%%%%%%%%%%%%%%
\begin{frame}{Costs to Probabilities}
\hh{Old trick:} Pass it through a sigmoid: \\
\vspace{0.25cm}
\begin{tabular}{rcl}
\textrm{cost} &=& $\theta \cdot \mathbf{f}$ \\
\pause
\textrm{weight} &=& $-\theta \cdot \mathbf{f}$ \\
\pause
\textrm{confidence} &=& $\frac{1}{1 + e^{- (-\theta \cdot \mathbf{f})}}$ \\
\end{tabular}
\vspace{0.5cm}
\pause

\hh{$\theta \geq 0$ means $\textrm{confidence} \in [0, \frac{1}{2}]$.}
\vspace{0.5cm}
\pause
  
\hh{Let $v$ be $1$ if search thinks the fact is true, $-1$ otherwise:} \\
\vspace{0.25cm}
\begin{center}
  $p(\textrm{true}) = \frac{v}{2} + \frac{1}{1 + e^{v \theta \cdot \mathbf{f}}}$.
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% LEARNING WEIGHTS
%%%%%%%%%%%%%%%%%%%
\newcommand\sD{\ensuremath{\mathcal{D}}}
\newcommand\bef{\ensuremath{\mathbf{f}}}
\def\header{
\hh{Input data $\sD = \{ (x, y) \}$.}
\begin{itemize}
  \item $x_i$ is a query fact.
  \item $y_i$ is the truth of that fact in $\{0, 1\}$.
\end{itemize}
\vspace{0.5cm}
\hh{Log Likelihood looks a lot like logistic regression...}
}
\newcommand\footer[3]{
\textcolor{#1}{...but, with our probability from before.} \\
\vspace{0.25cm}
\textcolor{#2}{...and with a log-barrier function.} \\
\vspace{0.25cm}
\hh{\textcolor[rgb]{#3}{Nonconvex objective; optimize anyways.}}
}
\def\title{Learning Costs}

\begin{frame}{\title}
\header
\begin{align*}
\l_\theta(\sD) = \sum_{0 \leq i < |\sD|} \Big[
    y_i \log \left(\frac{1}{1 + e^{-\theta \cdot \bef(x_i)}} \right)
    + (1 - y_i) \log \left(\frac{1}{1 + e^{\theta \cdot \bef(x_i)}} \right)
  \Big] \\
  \textcolor{white}{\left(\frac{1}{1 + e^{\theta \cdot \bef(x_i)}} \right)}
\end{align*}
\footer{white}{white}{1,1,1}
\end{frame}

\begin{frame}[noframenumbering]{\title}
\header
\begin{align*}
\l_\theta(\sD&) = \sum_{0 \leq i < |\sD|} \Big[
    y_i \log \left(\frac{v_i}{2} + \frac{1}{1 + e^{v_i \theta \cdot \bef(x_i)}} \right) \\
    &+ (1 - y_i) \log \left(\frac{-v_i}{2} + \frac{1}{1 + e^{-v_i \theta \cdot \bef(x_i)}} \right)
  \Big]
  \textcolor{white}{- \epsilon \log(\theta)}
\end{align*}
\footer{black}{white}{1,1,1}
\end{frame}

\begin{frame}[noframenumbering]{\title}
\header
\begin{align*}
\l_\theta(\sD&) = \sum_{0 \leq i < |\sD|} \Big[
    y_i \log \left(\frac{v_i}{2} + \frac{1}{1 + e^{v_i \theta \cdot \bef(x_i)}} \right) \\
    &+ (1 - y_i) \log \left(\frac{-v_i}{2} + \frac{1}{1 + e^{-v_i \theta \cdot \bef(x_i)}} \right)
  \Big]
  - \epsilon \log(\theta)
\end{align*}
\footer{black}{black}{1,1,1}
\end{frame}

\begin{frame}[noframenumbering]{\title}
\header
\begin{align*}
\l_\theta(\sD&) = \sum_{0 \leq i < |\sD|} \Big[
    y_i \log \left(\frac{v_i}{2} + \frac{1}{1 + e^{v_i \theta \cdot \bef(x_i)}} \right) \\
    &+ (1 - y_i) \log \left(\frac{-v_i}{2} + \frac{1}{1 + e^{-v_i \theta \cdot \bef(x_i)}} \right)
  \Big]
  - \epsilon \log(\theta)
\end{align*}
\footer{black}{black}{0.5,0,0}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% 
% FraCaS INTRO
%%%%%%%%%%%%%%%%%%%
\begin{frame}{Experiments}
\hh{FraCaS Textual Entailment Suite:}
\begin{itemize}
  \item Used in MacCartney and Manning (2007; 2008).
  \item RTE-style problems: is the hypothesis entailed from the premise? \\
  \pause
    \vspace{0.1cm}
    P: At least three commissioners spend a lot of time at home. \\
    H: \true{At least three commissioners spend time at home.} \\
    \vspace{0.1cm}
    \pause
    P: At most ten commissioners spend a lot of time at home. \\
    H: \false{At most ten commissioners spend time at home.}
    \vspace{0.1cm}
  \pause
  \item 9 focused sections; 3 in scope for this work.
\end{itemize}
\vspace{0.5cm}
\pause

\hh{\textbf{Not} a blind test set!}
\begin{itemize}
  \item ``Can we recover Natural Logic semantics without alignments?''
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% FraCaS RESULTS
%%%%%%%%%%%%%%%%%%%
\def\a#1{\textbf{#1}}
\begin{frame}{FraCaS Results}

\vspace{-1cm}
\hh{Systems}
\begin{itemize}
\item[] \textbf{M07}: MacCartney and Manning (2007)
\item[] \textbf{M08}: MacCartney and Manning (2008)
\item[] \blue{\textbf{N}: NaturalLI (this work)}
\end{itemize}

\begin{center}
  \begin{tabular}{llcccccccc}
    \hline
    $\mathsection$ & Category & \# & \multicolumn{2}{c}{Precision} & \multicolumn{2}{c}{Recall} & \multicolumn{3}{c}{Accuracy} \\
                   &          &       & \blue{N} & M08          & \blue{N} & M08          & \blue{N}  & M07 & M08 \\
    \hline
    \pause
                          % Pme           P07   Rme          R08   Ame         A07  A08
    \a{1} & \a{Quantifiers}  & \a{44} & \a{\blue{91 }}  & \a{95}  & \a{\blue{100}} & \a{100} & \a{\blue{95}} & \a{84} & \a{97} \\
    \pause
    \a{5} & \a{Adjectives}   & \a{15} & \a{\blue{80 }}  & \a{71}  & \a{\blue{66 }} & \a{83}  & \a{\blue{73}} & \a{60} & \a{80} \\
    \pause
    \a{6} & \a{Comparatives} & \a{16} & \a{\blue{90 }}  & \a{88}  & \a{\blue{100}} & \a{89}  & \a{\blue{87}} & \a{69} & \a{81} \pause \\
    \hline
\multicolumn{2}{l}{\a{Applicable (1,5,6)}}
                         & \a{75}     & \a{\blue{89}}   & \a{89}  & \a{\blue{94}}  & \a{94}  & \a{\blue{89}} & \a{76} & \a{90} \\
    \hline
  \end{tabular}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% ConceptNet INTRO
%%%%%%%%%%%%%%%%%%%
\begin{frame}{Experiments}
\hh{ConceptNet:}
\begin{itemize}
  \item A semi-curated collection of common-sense facts. \\
    \vspace{0.1cm}
    \pause
    \true{not all birds can fly} \\
    \true{noses are used to smell} \\
    \true{nobody wants to die} \\
    \true{music is used for pleasure}
    \vspace{0.1cm}
  \pause
  \item Negatives: ReVerb extractions marked false by Turkers.
  \pause
  \item Small (1378 train / 1080 test), but fairly broad coverage.
\end{itemize}
\vspace{0.5cm}
\pause

\hh{Our Knowledge Base:}
\begin{itemize}
  \item 270 million lemmatized Ollie extractions.
\end{itemize}
\vspace{0.5cm}
\pause

\hh{Baseline:} Simple lookup in the knowledge base.
\end{frame}
  
%%%%%%%%%%%%%%%%%%% 
% ConceptNet RESULTS
%%%%%%%%%%%%%%%%%%%
\begin{frame}{ConceptNet Results}
\begin{center}
  \begin{tabular}{l|cc:c|c}
    System             & P     & R    & F$_1$  & Accuracy \\
    \hline
    Direct Lookup      & 100.0 & 12.1 & 21.6 & 56.0 \\
    NaturalLI Only     & 88.8  & 40.1 & 55.2 & 67.5 \\
    NaturalLI + Lookup & 90.6  & 49.1 & 63.7 & 72.0  \\
  \end{tabular}
\end{center}
\pause

\begin{itemize}
  \item 4x improvement in Ollie recall.
  \pause
  \item Reasonably good precision.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCLUSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% 
% CONCLUSION
%%%%%%%%%%%%%%%%%%%
\begin{frame}{Conclusions}
\hh{Takeaways}
\begin{itemize}
  \item \textit{Deep} inferences from a \textit{large} knowledge base.
  \pause
  \item Leverage arbitrarily large plain-text knowledge bases.
  \pause
  \item ``Soft'' logic with probability of truth.
\end{itemize}
\vspace{0.5cm}
\pause

\hh{Strictly better than querying a knowledge base}
\begin{itemize}
  \item 12\% recall $\rightarrow$ 49\% recall @ 91\% precision
\end{itemize}
\vspace{0.25cm}
\pause

\hh{Strictly better fuzzy queries}
\begin{itemize}
  \item Checks logical entailment, not just \textit{fuzziness}
  \item Support doesn't have to be lexically similar
\end{itemize}
\pause
\end{frame}

%%%%%%%%%%%%%%%%%%% 
% THANKS
%%%%%%%%%%%%%%%%%%%
\begin{frame}[noframenumbering]{}
\begin{center}
  \hh{\huge{Thanks!}} \\
  \vspace{1cm}
  \includegraphics[width=3cm]{../../img/yarn-cat.jpg} \\
  \vspace{1cm}
  \url{http://plato42.stanford.edu/naturalli}
\end{center}
\end{frame}


\end{document}
