\Section{inference}{Inference As Search}
We run a search over mutations over specific words of the input candidate
  consequent.
The search problem is given a consequent as input.
This is composed of the following:

\begin{itemize}
  \item A sentence, tokenized into coherent logical entities.
        For instance, \textit{dog} and \textit{cat} would be entities;
          but, so would \textit{George W. Bush} -- despite the latter covering
          multiple words.
  \item A monotonicity marking for each of these words.
        This is one of: upward monotone, downward monotone, or flat (no
          monotonicity information could be gathered).
  \item The relevant sense of the word.
        This is a WordNet synset when it's available; if not, this is
          set to a special sense denoting no synset.
        If a WordNet synset is available, it will never be set to the
          `no synset' category.
\end{itemize}

The paths from the result of this searches are used
  as features in the learning problem
  (see \refsec{learn}).

We define the problem by specifying the state space,
  the valid transitions, along with the weights of
  the transitions.
The algorithm used is a Uniform Cost Search, leading
  to the additional challenge of ensuring that the
  cost of each transition is very fast -- that is,
  on the order of microseconds to keep pace with
  the memory accesses for the search.

%Before delving into the search problem, we define the
%  notion of a \textbf{node}.
%A node is, at a high level, a phrase such as one
%  which would correspond to an argument or a relation.
%It is:
%\begin{itemize}
%\item Case sensitive, but normalized to be either
%      capitalized, all capitals, or all lowercase.
%\item Lemmatized and normalized where appropriate.
%      For instance, \w{has} and \w{had} are the same
%      node. In particular, the ReVerb normalized
%      relation is used.
%\item In the case when the phrase maps exactly to a
%      word form in WordNet, it is the WordNet synset.
%\end{itemize}
%
%The definition of the search problem is:

\Subsection{state}{State}
A state in the graph is defined as a partial path from the consequent to a
  valid antecedent.
That is, a state is primarily parameterized by its candidate fact which may be
  in the knowledge base.
In addition, some auxilliary information is also tracked in a search state:

\begin{itemize}
  \item The fact this fact mutated from.
        This is necessary to reconstruct the final path.
  \item The monotonicity marking of each word in the fact.
        This is necessary for the weights, to ensure that monotonicity is
          respected.
  \item The synset of each word in the fact.
        This is necessary to compute the valid mutations of that edge, according
          to the WordNet hierarchy.
  \item A bitmask denoting whether a word has been modified in the past.
        This is set when a word has been modified, and then another word is
          subsequently modified.
        This ensures that edits on a word are contiguous.
        We do not edit a particualr word, and then edit another word, and then
          perform more edits on the initial word.
\end{itemize}

\Subsection{weights}{Weights}
\todo{This section is now wrong}
We can denote a state transition as $(n_{-1}, n)$ -- 
  corresponding to a state $n$ and the previous state $n-1$.
The weight of a transition from $(n_{-1},n)$ to
  $(n,n_{+1})$ is given by appealing to the
  transitions between $n_{-1}$ and $n$, and between
  $n$ and $n_{+1}$.

We denote the type of transition 
  (e.g., ins($w$) or freebase($r$))
  between $n_{-1}$ and $n$ to be $\phi_{-1}$, and the
  type of transition between $n$ and $n_{+1}$ to
  be $\phi_0$.
For both of these, we define weights $w_{\phi_{-1}}$
  and $w_{\phi_0}$, as well as a weight for the
  bigram of the two: $w_{\phi_{-1}, \phi_0}$.
Lastly, we incur a cost for exiting the domain of natural
  logic entailments: $w_{nle}$.
Our cost is thus given by:

\begin{equation}
\textrm{cost} = - w_{\phi_{0}}
                - w_{\phi_{-1}, \phi_0}
                - \1(\textrm{broke entailment}) \cdot w_{nle}
\end{equation}

We constrain $w_{\phi} \leq 0$ to ensure that the
  cost is always positive. 

Note, furthermore, that this is a natural decomposition
  of a log-linear model where $\phi$ denote features
  and $w$ denote the weights of the features.
The cost of a path will become:

\begin{equation}
\sum_i cost_i = - \sum_i \left[ w_{\phi_i)} + w_{\phi_{i-1}, \phi_i}\right]
\end{equation}

If we exponentiate the negative of this, we arrive at:

\begin{equation}
\exp( \sum_i cost_i )
  = \exp\left( \sum_i \left[ w_{\phi_i)} + w_{\phi_{i-1}, \phi_i} \right] \right)
\end{equation}

Over two classes: \textit{true} and \textit{false};
  and, defining notation where $\phi$ denotes the
  vector of active path types taken and $w$ denotes
  the global weight vector, we arrive at our
  log-linear model:

\begin{equation}
P(\textit{true}) \propto e^{w^{T} \phi}
\end{equation}

This decomposition is important, as it allows our
  search to get smarter along with our learning
  algorithm, and allows us to find better support
  for facts as we learn what patterns entail
  ``good'' support.

%\Subsection{boundaries}{Start and Terminal States}
%The start state is the relevant word from the fact
%  we are beginning from, normalized to a valid node
%  (i.e., lemmatized, properly cased, etc.).
%For example, the search from $a_1$ of \w{cats have tails}
%  would be \w{cat}.
%
%The terminal state of the search is any node which
%  is attached to a known fact on the correct term.
%For example, a search ffrom $a_1$ of \w{cats have tails}
%  would finish at \w{dog} if there is a fact in the
%  database such as \w{dogs have tails}.


\Subsection{transitions}{Transitions}
A valid transition is one between states
  $(n_{-1}, n)$ and $(n, n_{+1})$, such that in
  relation to $n$, $n_{+1}$ is one of:

\begin{itemize}
\item \textbf{Add/Remove a quantifier}.
      This is a class of transitions denoted by
      \textit{ins($w$)} or \textit{del($w$)} 
      where $w$ is the word being added or removed.
      For example, transitioning from \w{cat} to
      \w{a cat} or \w{every cat} or visa versa.
\item \textbf{Add/Remove an adjective}.
      Similar to above, but with adjectives. These are distinguished
      from the above in that they have meaning for natural logic
      entailment.
      \todo{how do we make this efficient?}
\item \textbf{WordNet hypernymy}.
      This is a class of transitions denoted by
      \textit{hyper} or \textit{hypo},
      For example, we could transition from \w{cat}
      to \w{feline} and eventually to \w{animal}
\item \textbf{WordNet relations}.
      This is a class of transitions related to the other
      WordNet relations (e.g., antonymy), primarily to
      capture some of the inferences in natlog.
\item \textbf{Freebase relations}.
      This is a class of transitions aimed primarily
      for proper nouns, traversing Freebase relations.
      These are denoted by \textit{freebase($r$)},
      where $r$ denotes the freebase relation we
      have traversed.
      For example, \w{Barack Obama} could transition
      to \w{USA} via the \textit{employee\_of}
      relation.
\item \textbf{ReVerb relation}.
      As in the Freebase case, we can move along a
      ReVerb relation, denoted as \textit{reverb($r$)}.
\item \textbf{Acronym and de-acronym}.
      Expanding or creating an acronym,denoted by
      \textit{acronym} and \textit{deacronym}.
\item \textbf{Nearest neighbor similarity}.
      The fallback is to search for nearest neighbors in similarity space.
      This is, roughly, the equivalent of the CoNLL paper.
\item \textbf{Drop sense}. Drop the sense of a word -- this is necessary to
      begin operating in nearest neighbors space, or even with Freebase, etc.
      relations.
\item \textbf{Infer sense}. Go from a sense-less word to one of its word senses.
      For example, chosing a particular sense for \textit{cat} from the
        initially senseless definition.
\end{itemize}

\note{Negation does strange things; flip "truth state" in forwards and reverse cases}

\Subsection{jc}{Generalization of JC Similarity}
Let us assume we have words $w_1$ and $w_2$, with a least common subsumer $\textrm{lcs}$.
The JC distance $\textrm{dist}_{\textrm{jc}}(w_1, w_2)$ is:

\begin{equation}
\textrm{dist}_{\textrm{jc}}(w_1, w_2)
  = \log\frac{p(\textrm{lcs})^2}{p(w_1)p(w_2)}
\end{equation}

We show that our search over the Wordnet hierarchy generalizes this similarity.
In particular, let us define two features, $\phi_\uparrow$ and $\phi_\downarrow$,
  corresponding to going up and down the WordNet hierarchy, respectively.
Traversing the Wordnet hierarchy from words $w \rightarrow w'$ thus fires the $\phi$
  features with counts:

\begin{align}
  \phi_\uparrow(w \rightarrow w')
    &= \log\frac{p(w')}{p(w)} = \log p(w') - \log p(w) \\
  \phi_\downarrow(w \rightarrow w')
    &= \log\frac{p(w)}{p(w')} = \log p(w) - \log p(w') 
\end{align}

We now introduce weights associated with each of these two operations, denoted
  $\theta_\uparrow$ and $\theta_\downarrow$, for each pair of words participating
  in a WordNet edge.
The score of a path is then defined as the dot product of the weights and features
  as described above: $\theta^{\textrm{T}}\phi$.

We can factorize this along the path
  $w_1, w_1^{(1)}, w_1^{(2)}, \dots, \textrm{lcs}, \dots, w_2^{(2)},  w_2^{(1)}, w_2$
  as follows:

\begin{align*}
\theta^{\textrm{T}}\phi
  &= \theta_\uparrow \left( 
    \left[\log p(w_1^{(1)}) - \log p(w_1)\right] +
    \dots +
    \left[\log p(w_1^{(n)}) - \log p(\textrm{lcs})\right]
    \right) + \\
  &~~ \theta_\downarrow \left( 
    \left[\log p(\textrm{lcs}) - \log p(w_1^{(n)}) \right] +
    \dots +
    \left[\log p(w_1) - \log p(w_1^{(1)})\right]
    \right) \\
  &= \theta_\uparrow \left( \log \frac{p(\textrm{lcs}}{p(w_1)} \right) +
     \theta_\downarrow \left( \log \frac{p(\textrm{lcs}}{p(w_2)} \right) \\
  &= \log \frac{ p(\textrm{lcs})^{\theta_\uparrow + \theta_\downarrow} }
               { p(w_1)^{\theta_\uparrow} + p(w_2)^{\theta_\downarrow} }
\end{align*}

Note that setting both $\theta_\uparrow$ and $\theta_\downarrow$ to 1 exactly
  yield JC similarity.
