\Section{learning}{Learning Transition Costs}
The learning task can be viewed as a constrained optimization problem.
Subject to the constraint that all elements of the cost vector \btheta\ 
  must be non-negative, we optimize the probability from
  Equation \refeqn{prob} compared against the gold annotation.
As training data, we are given a number of facts, annotated with a
  truth value of \textit{true} or \textit{false}.
We assume that all facts in our database are true; therefore,
  $p(\textrm{valid})$ corresponds directly to $p(\textrm{true})$.

We learn costs using an iterative algorithm.
At each iteration, we take the costs from the previous iteration
  and run search over every example to obtain candidate paths.
At each iteration, analogous to logistic regression,
  the log-likelihood of the training data becomes:

\begin{align*}
\l\l_\theta(\sD&) = \sum_{0 \leq i < |\sD|} \Big[
    y_i \log \left(\frac{v_i}{2} + \frac{1}{1 + e^{v_i \theta \cdot f(x_i)}} \right) \\
    &+ (1 - y_i) \log \left(\frac{-v_i}{2} + \frac{1}{1 + e^{-v_i \theta \cdot f(x_i)}} \right)
  \Big],
\end{align*}

\noindent where $y_i$ is 1 if the example is annotated true, and 0
  otherwise, $f(x_i)$ are the features extracted for path $i$, and
  $v_i$ is the inference state predicted in search, as in
  \refsec{inference-prob}.
The objective function becomes our negative log-likelihood, in addition
  to an $l_2$ regularization term, and a log barrier function to 
  prohibit negative costs:

\begin{equation*}
O(\sD) = -\l\l_\theta(\sD) 
  + \frac{1}{2 \sigma^2}\|\theta\|_2^2
  - \epsilon \log(-\theta).
\end{equation*}

\noindent We set $\sigma=100$ and $\epsilon=1e^{-5}$.

Although this objective is non-convex, there is a natural initialization
  of costs to match intuitions about Natural Logic.
We initialize the first iteration to these costs, and run
  conjugate gradient descent to convergence on a local minima
  on each subsequent iteration.
Lastly, after each iteration, the cost for any feature which has
  never fired during training is decreased by half.
This is to encourage the search to take these expensive edges, and
  learn an appropriate cost for them.



%We learn costs using an iterative algorithm.
%At each iteration, we take the costs from the previous iteration
%  and run the derivation search over every example, providing
%  a predicted probability of truth for each query fact.
%Optimizing the likelihood of the training data according to
%  Equation \refeqn{prob} is impractical as the objective is nonconvex;
%  the rest of today is intended to be spent on implementing a piecewise
%  optimization to mitigate this.
%A simple heuristic (weight = 1 - \# times this feature fired in good examples
%  / \# of times feature fired total) does ok, but is a little braindead.
%
%In particular, we construct a dataset of $(x,y)$ pairs, where $x$ is the
%  featurized path for a particular example, and $y$ is whether the
%  path ended in a correct state.
%A correct state is defined trivially between \textit{valid} and
%  \textit{invalid}.
%In cases where the gold annotation contains a state of 
%  \textit{unknown validity}, any prediction is marked incorrect.
%
%The resulting weights -- in one-to-one correspondence with the costs
%  for our search -- are then negated and normalized via a bilinear
%  transform to fall between 0 and $-5$, with a mean of $-1$.
%A default weight of $-2$ is assigned for any feature which did not
%  appear in the training data, excepting the case where a previous
%  iteration of the algorithm had set a value for that weight, in
%  which case the previous value is used.
