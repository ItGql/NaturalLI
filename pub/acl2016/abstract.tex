\begin{abstract}
Broad domain question answering is often difficult in the absence of
  structured knowledge bases, and can benefit from
  shallow lexical methods (broad coverage) and logical
  reasoning (high precision).
%  from signals from
%  both shallow lexical methods and formal logical
%  reasoning.
We propose an approach for incorporating both of these signals in
  a unified framework based on natural logic.
We extend the breadth of inferences afforded by natural
  logic to include relational entailment
  (e.g., \w{buy} $\rightarrow$ \w{own}) 
  and meronymy
  (e.g., a person born in a city is born the city's country).
Furthermore, we train an \textit{evaluation function} -- akin
  to gameplaying -- to evaluate the expected truth of
  candidate premises on the fly.
%%  while searching over candidate premises to capture
%  to provide
%  a soft judgment for whether that premise is supported
%  by the knowledge base.
We evaluate our approach on answering
  multiple choice science questions, achieving strong
  results on the dataset.

%Natural logic question answering systems, such as NaturalLI,
%  can capture a large number of useful inferences; nonetheless,
%  there are many cases that are out of scope for the
%  formalism.
%We propose two contributions to mitigate these cases:
%  we incorporate reasoning about relational entailment
%  (e.g., \w{buy} $\rightarrow$ \w{own}) 
%  and meronymy
%  (e.g., a person born in a city is born the city's country).
%Furthermore, we incorporate an \textit{evaluation function}
%  while searching over candidate premises that captures
%  a soft judgment for whether that premise is supported
%  by the knowledge base.
%This allows us to provide a confidence for every query.
%We present
%  the best published results on the Aristo science exams corpus.



%while searching for candidate premises, backoff to
%  a lexical classifier for a soft judgment on whether
%  the candidate is entailed by a known premise.
%Simple lexical methods are often surprisingly effective at
%  solving entailment problems.
%However, they are easily thrown off by lexically subtle logical
%  fallacies -- e.g., negation.
%We present an approach for combining logical reasoning with
%  Natural Logic with a broad-coverage but lower-precision
%  lexical classifier, and show that this combination outperforms
%  either system in isolation.
%We evaluate our system in a question answering setting, where
%  a large set of candidate premises can confirm or contradict
%  the truth of a query.
%We validate that our system maintains strict entailments on FraCaS,
\end{abstract}
