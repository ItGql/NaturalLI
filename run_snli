#!/bin/bash
exec scala -cp qry.jar:test/src/test_client.jar:src/naturalli_client.jar "$0" "$@"
!#
import Qry._

val SNLI_TRAIN = "etc/snli/snli_1.0rc3_train.tab"
val SNLI_TEST = "etc/snli/snli_1.0rc3_dev.tab"

val SICK_TRAIN = "etc/sick/sick_train.tab"
val SICK_SNLI_TRAIN = "etc/sick/sick+snli_train.tab"
val SICK_TEST = "etc/sick/sick_dev.tab"

val RTE_TRAIN = "etc/RTE-3/English_dev.tab"
val RTE_SNLI_TRAIN = "etc/sick/rte+snli_train.tab"
val RTE_TEST = "etc/RTE-3/English_dev.tab"


//
// Define the program
//
def program(memoryMB:Int):List[String] = ("java"
  -('cp, cp)
//	-"Xrunhprof:cpu=samples,depth=25"
	-("Xmx" + memoryMB + "M")
	-"Xss32m"
  -"XX:MaxPermSize=256M"
  -"Dwordnet.database.dir=etc/WordNet-3.1/dict"
	-'server
  -'ea
	->"edu.stanford.nlp.naturalli.entail.ClassifierTrainer"
  -("classifier", "SIMPLE")
  -("naturalli_search", "/dev/null")
  -("test.errors", touch("errors.txt"))
).toList

using("runs/")


////
//// Run the learning curve
////
//parallel(8) submit(program(8500)
//  -("train.file", SNLI_TRAIN)
//  -("train.cache.do", "false")
//  -("test.file", SNLI_TEST)
//  -("test.cache.do", "false")
//
//  -("train.count", "10" & "100" & "1000" & "10000" & "50000" & "100000" & "250000" & "1000000")
//  -("model", touch("model.ser.gz"))
//  -("features", """[ BLEU  LENGTH_DIFF  OVERLAP  POS_OVERLAP  ENTAIL_UNIGRAM  ENTAIL_BIGRAM  CONCLUSION_NGRAM ]""" )
//  -("features.nolex", "true" & "false")
//
//  -("parallel", "false")
//  
//  -("log.file", touch("redwood.log"))
//)
//
////
//// Run the unigram learning curve
////
//parallel(8) submit(program(8500)
//  -("train.file", SNLI_TRAIN)
//  -("train.cache.do", "false")
//  -("test.file", SNLI_TEST)
//  -("test.cache.do", "false")
//
//  -("train.count", "10" & "100" & "1000" & "10000" & "50000" & "100000" & "250000" & "1000000")
//  -("model", touch("model.ser.gz"))
//  -("features", """[ BLEU  LENGTH_DIFF  OVERLAP  POS_OVERLAP  ENTAIL_UNIGRAM ]""" )
//  -("features.nolex", "false")
//
//  -("parallel", "false")
//  
//  -("log.file", touch("redwood.log"))
//)

//
// Try out other features
//
parallel(2) submit(program(32000)
  -("train.file", SNLI_TRAIN)
  -("train.cache.do", "false")
  -("test.file", SNLI_TEST)
  -("test.cache.do", "false")

  -("train.count", "1000000")
  -("model", touch("model.ser.gz"))
  -("features", 
//      """[ BLEU  LENGTH_DIFF  OVERLAP  POS_OVERLAP  ENTAIL_UNIGRAM  ENTAIL_BIGRAM  CONCLUSION_NGRAM ENTAIL_BOTH_GRAM ]""" &
//      """[ BLEU  LENGTH_DIFF  OVERLAP  POS_OVERLAP  ENTAIL_UNIGRAM  ENTAIL_BIGRAM  CONCLUSION_NGRAM ENTAIL_TRIGRAM ]""" &
//      """[ BLEU  LENGTH_DIFF  OVERLAP  POS_OVERLAP  ENTAIL_UNIGRAM  ENTAIL_BIGRAM  CONCLUSION_NGRAM ENTAIL_TRIGRAM ENTAIL_BOTH_GRAM ]""" &
      """[ BLEU  LENGTH_DIFF  OVERLAP  POS_OVERLAP  ENTAIL_UNIGRAM  ENTAIL_BIGRAM  CONCLUSION_NGRAM ]"""
      )
  -("features.nolex", "false")
  -("parallel", "false")
  -("log.file", touch("redwood.log"))
)

////
//// Run ablation studies with keyword features
////
//parallel(1) submit(program(70000)
//  -("train.file", SNLI_TRAIN)
//  -("train.cache.do", "false")
//  -("test.file", SNLI_TEST)
//  -("test.cache.do", "false")
//
//  -("train.count", "10" & "100" & "1000" & "10000" & "50000" & "100000" & "250000" & "1000000")
//  -("model", touch("model.ser.gz"))
//  -("features", """[ BLEU  LENGTH_DIFF  OVERLAP  POS_OVERLAP  ENTAIL_UNIGRAM  ENTAIL_BIGRAM  CONCLUSION_NGRAM  KEYWORD_OVERLAP  ENTAIL_KEYWORD ]""" )
//  -("features.nolex", "true" & "false")
//
//  -("parallel", "false")
//  
//  -("log.file", touch("redwood.log"))
//)
//
////
//// Run the SICK data
////
//parallel(8) submit(program(8500)
//  -("train.file", SICK_TRAIN & SNLI_TRAIN & SICK_SNLI_TRAIN)
//  -("train.cache.do", "false")
//  -("test.file", SICK_TEST)
//  -("test.cache.do", "false")
//
//  -("train.count", "10000")
//  -("model", touch("model.ser.gz"))
//  -("features", """[ BLEU  LENGTH_DIFF  OVERLAP  POS_OVERLAP  ENTAIL_UNIGRAM  ENTAIL_BIGRAM  CONCLUSION_NGRAM ]""" )
//  -("features.nolex", "true" & "false")
//
//  -("parallel", "false")
//  
//  -("log.file", touch("redwood.log"))
//)
//
////
//// Run the RTE data
////
//parallel(8) submit(program(8500)
//  -("train.file", RTE_TRAIN & SNLI_TRAIN & RTE_SNLI_TRAIN)
//  -("train.cache.do", "false")
//  -("test.file", SICK_TEST)
//  -("test.cache.do", "false")
//
//  -("train.count", "10000")
//  -("model", touch("model.ser.gz"))
//  -("features", """[ BLEU  LENGTH_DIFF  OVERLAP  POS_OVERLAP  ENTAIL_UNIGRAM  ENTAIL_BIGRAM  CONCLUSION_NGRAM ]""" )
//  -("features.nolex", "true" & "false")
//
//  -("parallel", "false")
//  
//  -("log.file", touch("redwood.log"))
//)


def cp:String = {
  val JAVANLP = List(
    System.getenv("JAVANLP_HOME") + "/projects/core/classes",
    System.getenv("JAVANLP_HOME") + "/projects/core/lib/joda-time.jar",
    System.getenv("JAVANLP_HOME") + "/projects/core/lib/jollyday-0.4.7.jar",
    System.getenv("JAVANLP_HOME") + "/projects/core/lib/protobuf.jar",
    System.getenv("JAVANLP_HOME") + "/projects/more/classes",
    System.getenv("JAVANLP_HOME") + "/projects/more/lib/BerkeleyParser.jar",
    System.getenv("JAVANLP_HOME") + "/projects/research/classes",
    System.getenv("JAVANLP_HOME") + "/projects/research/lib/reverb.jar",
    System.getenv("JAVANLP_HOME") + "/projects/research/lib/postgresql.jar",
    "/u/nlp/data/StanfordCoreNLPModels/stanford-corenlp-models-current.jar",
    "/u/nlp/data/StanfordCoreNLPModels/stanford-corenlp-caseless-models-current.jar"
  ).mkString(":")
  val SCALA = List(
      System.getenv("SCALA_HOME") + "/lib/scala-library.jar",
      System.getenv("SCALA_HOME") + "/lib/config-1.2.0.jar",
      System.getenv("SCALA_HOME") + "/lib/scala-xml_2.11-1.0.1.jar"
    ).mkString(":")
  val CUSTOM = List(
      "lib/corenlp-scala.jar",
      "lib/trove.jar",
      "lib/jaws.jar",
      "lib/scripts/sim-1.0.jar",
      "lib/gson-2.3.1.jar",
      "lib/protobuf.jar"
    ).mkString(":")
  List("src/naturalli_preprocess.jar", JAVANLP, SCALA, CUSTOM).mkString(":")
}
